#!/bin/bash

echo "=========================================="
echo "S3 Hybrid Storage Deployment"
echo "=========================================="
echo ""

# Color codes
GREEN='\033[0;32m'
YELLOW='\033[1;33m'
RED='\033[0;31m'
NC='\033[0m' # No Color

echo -e "${YELLOW}Step 1: Installing pyarrow dependency...${NC}"
pip3 install pyarrow>=14.0.0 || echo -e "${RED}Warning: pip installation failed${NC}"

# Install in backend virtualenv if it exists
if [ -d "/opt/ai-search/backend/venv" ]; then
    echo "Installing in backend virtualenv..."
    /opt/ai-search/backend/venv/bin/pip install pyarrow>=14.0.0 || echo -e "${RED}Warning: venv installation failed${NC}"
elif [ -d "backend/venv" ]; then
    echo "Installing in backend virtualenv (relative path)..."
    backend/venv/bin/pip install pyarrow>=14.0.0 || echo -e "${RED}Warning: venv installation failed${NC}"
fi
echo ""

echo -e "${YELLOW}Step 2: Pulling latest code...${NC}"
git pull origin claude/evaluate-html-to-react-011CV429JWyi22JWAMhZUBox
echo ""

echo -e "${YELLOW}Step 3: Verifying AWS credentials and S3 access...${NC}"
echo "Checking if EC2 IAM role can access S3..."
aws s3 ls s3://plfs-han-ai-search/public_company_tracker/ 2>&1 | head -5
if [ $? -eq 0 ]; then
    echo -e "${GREEN}âœ“ S3 access confirmed${NC}"
else
    echo -e "${RED}âœ— Cannot access S3 bucket${NC}"
    echo "Make sure EC2 IAM role has s3:PutObject, s3:GetObject, s3:ListBucket permissions"
fi
echo ""

echo -e "${YELLOW}Step 4: Checking SQLite database size...${NC}"
DB_PATH="/opt/ai-search/data/db/stocks.db"
if [ -f "$DB_PATH" ]; then
    DB_SIZE=$(du -h "$DB_PATH" | cut -f1)
    DB_RECORDS=$(sqlite3 "$DB_PATH" "SELECT COUNT(*) FROM stock_daily;" 2>/dev/null || echo "N/A")
    echo "Current SQLite database:"
    echo "  Size: $DB_SIZE"
    echo "  Records: $DB_RECORDS"
else
    echo "Database not found at $DB_PATH"
fi
echo ""

echo -e "${YELLOW}Step 5: Restarting backend with S3 storage support...${NC}"
pm2 restart ai-search-backend
sleep 5
echo ""

echo -e "${YELLOW}Step 6: Checking backend logs for S3 initialization...${NC}"
pm2 logs ai-search-backend --lines 10 --nostream | grep -i "scheduler\|storage" || echo "Backend started successfully"
echo ""

echo -e "${YELLOW}Step 7: Testing S3 storage functionality...${NC}"
echo "Testing S3 list operation..."
python3 -c "
from backend.app.services.s3_storage import S3StockDataService
s3 = S3StockDataService()
tickers = s3.list_archived_tickers(is_hkex=True)
print(f'Found {len(tickers)} archived HKEX tickers in S3' if tickers else 'No archived data yet (expected for new deployment)')
" 2>&1 | head -3
echo ""

echo "=========================================="
echo "Summary of Changes"
echo "=========================================="
echo ""
echo "New Hybrid Storage Architecture:"
echo "  ðŸ“Š Recent data (last 1 year): SQLite (fast local access)"
echo "  â˜ï¸  Old data (> 1 year): S3 (scalable cloud storage)"
echo ""
echo "Files created:"
echo "  - backend/app/services/s3_storage.py"
echo "  - archive_to_s3.py (manual archival script)"
echo ""
echo "Files modified:"
echo "  - backend/app/services/stock_data.py (hybrid query logic)"
echo "  - backend/app/services/scheduler.py (weekly archival job)"
echo "  - requirements.txt (added pyarrow)"
echo ""
echo "S3 Storage Locations:"
echo "  - HKEX 18A: s3://plfs-han-ai-search/public_company_tracker/hkex_18a_stocks/"
echo "  - Portfolio: s3://plfs-han-ai-search/public_company_tracker/portfolio_comps_tracker/"
echo ""
echo "Data Format:"
echo "  - Format: Apache Parquet (compressed, efficient)"
echo "  - Partitioning: By year/month (e.g., 2561.HK/2024/01.parquet)"
echo "  - Compression: Snappy (fast compression/decompression)"
echo ""
echo "Archival Schedule:"
echo "  â° Automatic: Every Sunday at 2:00 AM"
echo "  ðŸ”„ Manual: Run 'python3 archive_to_s3.py' anytime"
echo ""
echo "Benefits:"
echo "  âœ“ Charts always show full 1 year of data (from SQLite)"
echo "  âœ“ Older data (>1 year) archived to S3 automatically"
echo "  âœ“ Unlimited historical data storage"
echo "  âœ“ Cost-effective (S3 is $0.023/GB/month)"
echo "  âœ“ Automatic backups in S3"
echo "  âœ“ Charts automatically fetch from S3 when needed"
echo ""
echo "Next Steps:"
echo "  1. To archive existing data now: python3 archive_to_s3.py"
echo "  2. Or wait for automatic archival (Sunday 2 AM)"
echo "  3. Monitor logs: pm2 logs ai-search-backend"
echo ""
echo "Deployment complete!"
