{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf95bfd7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#%%capture --no-stderr\n",
    "%run docIndex.ipynb\n",
    "#!pip install --upgrade openai\n",
    "!pip install bitsandbytes\n",
    "!pip install accelerate\n",
    "!pip install rank_bm25\n",
    "!pip install markdown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cba4b1ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import argparse\n",
    "import openai\n",
    "from openai import OpenAI\n",
    "\n",
    "from langchain import PromptTemplate\n",
    "from botocore.exceptions import ClientError\n",
    "\n",
    "# import sagemaker\n",
    "# from sagemaker import get_execution_role\n",
    "\n",
    "from langchain.llms import BaseLLM\n",
    "from ipywidgets import Dropdown\n",
    "# from sagemaker.jumpstart.notebook_utils import list_jumpstart_models\n",
    "\n",
    "import markdown\n",
    "\n",
    "from collections import Counter\n",
    "import re\n",
    "import os, gc, torch\n",
    "\n",
    "from datetime import datetime\n",
    "import pytz\n",
    "import pickle\n",
    "import json\n",
    "\n",
    "import chromadb \n",
    "from chromadb import Settings\n",
    "from langchain.embeddings.base import Embeddings\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "import requests\n",
    "\n",
    "# from langchain.vectorstores import Chroma\n",
    "from langchain_chroma import Chroma\n",
    "#from langchain_community.vectorstores import Chroma   # instead of langchain_chroma\n",
    "\n",
    "from transformers import set_seed\n",
    "\n",
    "# from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "\n",
    "from transformers.utils import is_bitsandbytes_available\n",
    "\n",
    "from langchain.retrievers import BM25Retriever\n",
    "from langchain.retrievers.ensemble import EnsembleRetriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9cf64f3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#run this for the first time to process pdfs, no need to run again after that except there are new files added\n",
    "s3_bucket = 'plfs-han-llm-experiment'\n",
    "s3_source_folder = 'factsheet-generation/PPInnova/internal/jarvis_docs/'\n",
    "s3_output_folder = 'factsheet-generation/PPInnova/internal/jarvis_docs/'\n",
    "process_pdfs_in_s3_folder(s3_bucket, s3_source_folder, s3_bucket, s3_output_folder, model=\"o4-mini\")  #gpt-5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73e258cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Open and save the PDF to potentially fix issues\n",
    "# doc1 = fitz.open(\"Euhearing_sharepoint_selected_20250921--01 From company--项目相关资料包-Euhearing_20250817091531.pdf\")\n",
    "# doc1.save(\"Euhearing_sharepoint_selected_20250921--01 From company--项目相关资料包-Euhearing_20250817091531_repaired.pdf\", garbage=4, deflate=True, clean=True)\n",
    "# doc1.close()\n",
    "\n",
    "# # # Now try with the repaired version\n",
    "# doc1 = fitz.open(\"Euhearing_sharepoint_selected_20250921--01 From company--项目相关资料包-Euhearing_20250817091531_repaired.pdf\")\n",
    "# text1 = doc1[1].get_text()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64d59906",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#if there are new files added, run this to process unprocessed pdfs\n",
    "s3_bucket = 'plfs-han-llm-experiment'\n",
    "s3_source_folder = 'factsheet-generation/Euhearing/internal/jarvis_docs/'\n",
    "s3_output_folder = 'factsheet-generation/Euhearing/internal/jarvis_docs/'\n",
    "process_unprocessed_pdfs_in_s3_folder(s3_bucket, s3_source_folder, s3_bucket, s3_output_folder, model=\"o4-mini\")  #gpt-5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "246458c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def copy_files_flattened_and_renamed_within_s3(bucket_name, source_prefix, destination_prefix):\n",
    "    \n",
    "    s3 = boto3.client('s3')\n",
    "    response = s3.list_objects_v2(Bucket=bucket_name, Prefix=source_prefix)\n",
    "    if 'Contents' not in response:\n",
    "        print(f\"No files found in {source_prefix}\")\n",
    "        return\n",
    "\n",
    "    for obj in response['Contents']:\n",
    "        source_key = obj['Key']\n",
    "        if source_key.endswith('/'):\n",
    "            continue  # Skip directories\n",
    "\n",
    "        # Extract the relative path from source_prefix\n",
    "        relative_path = os.path.relpath(source_key, source_prefix)\n",
    "        \n",
    "        # Extract the top folder name\n",
    "        top_folder_name = relative_path.split('/')[0]\n",
    "        \n",
    "        # Extract the original file name\n",
    "        original_file_name = os.path.basename(source_key)\n",
    "        \n",
    "        # Create the new file name with the folder name included\n",
    "        new_file_name = f\"{top_folder_name}_{original_file_name}\"\n",
    "        \n",
    "        # Construct the destination key\n",
    "        destination_key = f\"{destination_prefix.rstrip('/')}/{new_file_name}\"\n",
    "        \n",
    "        # Copy the object to the new location\n",
    "        copy_source = {'Bucket': bucket_name, 'Key': source_key}\n",
    "        s3.copy_object(CopySource=copy_source, Bucket=bucket_name, Key=destination_key)\n",
    "        \n",
    "        print(f\"Copied {source_key} to {destination_key}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d50de64a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# s3_bucket = 'plfs-han-llm-experiment'\n",
    "# dir1 = '2pager/Oculis/docs/'\n",
    "# dir2 = '2pager/Oculis/internal/'\n",
    "# # dir2 = '2pager/Mythic/internal/jarvis_docs/'\n",
    "# # dir3 = '2pager/Mythic/internal/jarvis_tables/'\n",
    "# # copy_files_from_s3_to_s3(s3_bucket, dir2, s3_bucket, dir1)\n",
    "# # copy_files_from_s3_to_s3(s3_bucket, dir3, s3_bucket, dir1)\n",
    "\n",
    "# copy_files_flattened_and_renamed_within_s3(s3_bucket, dir2, dir1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdec237f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# %%capture --no-stderr\n",
    "# !pip uninstall -y torch torchvision\n",
    "# # !pip install torch torchvision --index-url https://download.pytorch.org/whl/cu117\n",
    "# !pip install --upgrade torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118\n",
    "# import torchvision.transforms as transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3428507f",
   "metadata": {},
   "outputs": [],
   "source": [
    "## RecursiveCharacterTextSplitter\n",
    "def process_documents_from_s3(s3_bucket: str, s3_folder: str, ignored_files: List[str] = []) -> List[Document]:\n",
    "    \"\"\"\n",
    "    Load documents from S3 and split in chunks\n",
    "    \"\"\"\n",
    "    print(f\"Loading documents from s3://{s3_bucket}/{s3_folder}\")\n",
    "    documents = load_documents_from_s3(s3_bucket, s3_folder, ignored_files)\n",
    "    if not documents:\n",
    "        print(\"No new documents to load\")\n",
    "        exit(0)\n",
    "    print(f\"Loaded {len(documents)} new documents from s3://{s3_bucket}/{s3_folder}\")\n",
    "    \n",
    "    #the chunk_size parameter in RecursiveCharacterTextSplitter refers to the number of characters\n",
    "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=chunk_size, chunk_overlap=chunk_overlap)\n",
    "    texts = text_splitter.split_documents(documents)\n",
    "    \n",
    "    # Add filename to each chunk after splitting\n",
    "    for chunk in texts:\n",
    "        filename = chunk.metadata.get('source', 'unknown_file')\n",
    "        chunk.page_content = f\"From file: {filename}\\n\\n{chunk.page_content}\"\n",
    "    \n",
    "    print(f\"Split into {len(texts)} chunks of text (max. {chunk_size} tokens each)\")\n",
    "    return texts\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca6ad97d",
   "metadata": {},
   "outputs": [],
   "source": [
    "USE_INT8 = True          # flip to True after `pip install bitsandbytes`\n",
    "embeddings_model_name = \"Qwen/Qwen3-Embedding-4B\"\n",
    "#RecursiveCharacterTextSplitter, size of chars\n",
    "chunk_size = 1000\n",
    "chunk_overlap = 200\n",
    "\n",
    "\n",
    "class SentenceTransformerEmbeddings(Embeddings):\n",
    "    def __init__(self,\n",
    "                 model_name: str,\n",
    "                 device: str = \"cpu\",\n",
    "                 batch_size: int = 8,\n",
    "                 max_length: int = 8192,):\n",
    "        self.device     = device\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "        # minimise CUDA fragmentation\n",
    "        os.environ.setdefault(\n",
    "            \"PYTORCH_CUDA_ALLOC_CONF\",\n",
    "            \"expandable_segments:True,max_split_size_mb:128\"\n",
    "        )\n",
    "        gc.collect(); torch.cuda.empty_cache()\n",
    "\n",
    "        # --------------------- build kwargs ------------------------------\n",
    "        load_kwargs = {\"trust_remote_code\": True}\n",
    "        if device == \"cuda\":\n",
    "            if USE_INT8 and is_bitsandbytes_available():\n",
    "                load_kwargs[\"model_kwargs\"] = {\n",
    "                    \"load_in_8bit\": True,\n",
    "                    \"device_map\": \"auto\",\n",
    "                }                \n",
    "            else:\n",
    "                load_kwargs[\"model_kwargs\"] = {\"torch_dtype\": torch.float16}\n",
    "\n",
    "        # --------------------- load model -------------------------------\n",
    "        self.model = SentenceTransformer(model_name, **load_kwargs)\n",
    "\n",
    "        # If we took the fp16 path, cast & move once\n",
    "        if device == \"cuda\" and not (USE_INT8 and is_bitsandbytes_available()):\n",
    "            self.model.half()          # weights → fp16\n",
    "            self.model.to(device)      # onto GPU\n",
    "\n",
    "    # --------------------- LangChain hooks ------------------------------\n",
    "    def embed_documents(self, documents: List[str]) -> List[List[float]]:\n",
    "        vecs = self.model.encode(\n",
    "            documents,\n",
    "            batch_size=self.batch_size,\n",
    "            convert_to_numpy=True,\n",
    "            show_progress_bar=False,\n",
    "            device=self.device,\n",
    "        )\n",
    "        return vecs.tolist()\n",
    "\n",
    "    def embed_query(self, query: str) -> List[float]:\n",
    "        return self.embed_documents([query])[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf27049a",
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6f4985e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\n",
    "set_seed(42)\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "embeddings = HuggingFaceEmbeddings(\n",
    "    model_name=embeddings_model_name,\n",
    "    model_kwargs={'device': DEVICE},\n",
    "    encode_kwargs={'batch_size': 8, 'normalize_embeddings': True}\n",
    ")\n",
    "\n",
    "# ChromaDB setup - this should work now\n",
    "current_dir = os.getcwd()\n",
    "persist_directory_jd = os.path.join(current_dir, \"chroma_db_jarvis_docs\")\n",
    "os.makedirs(persist_directory_jd, exist_ok=True)\n",
    "chroma_client_jd = chromadb.PersistentClient(path=persist_directory_jd)\n",
    "\n",
    "# Drop & recreate collection \n",
    "try:\n",
    "    chroma_client_jd.delete_collection(\"jarvis_docs\")\n",
    "except ValueError:\n",
    "    pass\n",
    "\n",
    "db_jd = Chroma(\n",
    "    collection_name=\"jarvis_docs\",\n",
    "    embedding_function=embeddings,  # Using external embeddings\n",
    "    client=chroma_client_jd,\n",
    "    collection_metadata={\"hnsw:space\": \"cosine\"},\n",
    ")\n",
    "\n",
    "\n",
    "s3_bucket = \"plfs-han-llm-experiment\"\n",
    "s3_folder = 'factsheet-generation/PPInnova/internal/jarvis_docs/'\n",
    "ignored_files = []\n",
    "texts = process_documents_from_s3(s3_bucket, s3_folder, ignored_files)\n",
    "\n",
    "CHROMA_BATCH_SIZE = 1000\n",
    "print(f\"Total documents to process: {len(texts)} (batch {CHROMA_BATCH_SIZE})\")\n",
    "\n",
    "for i in range(0, len(texts), CHROMA_BATCH_SIZE):\n",
    "    batch_texts = texts[i : i + CHROMA_BATCH_SIZE]\n",
    "    print(f\"Adding docs {i}-{i+len(batch_texts)-1}\")\n",
    "    db_jd.add_documents(batch_texts)\n",
    "    \n",
    "    if DEVICE == \"cuda\":\n",
    "        torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "\n",
    "print(\"Vector database creation completed successfully!\")\n",
    "print(\"Final document count:\", chroma_client_jd.get_collection(\"jarvis_docs\").count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "229c7c89",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "\n",
    "# os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\n",
    "# set_seed(42)\n",
    "\n",
    "# DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# embeddings = SentenceTransformerEmbeddings(\n",
    "#     model_name = embeddings_model_name,\n",
    "#     device     = DEVICE,\n",
    "#     batch_size = 16,\n",
    "#     max_length = 8192\n",
    "# )\n",
    "\n",
    "# # ───────────────────────────  CHROMA SETUP ─────────────────────────\n",
    "# current_dir            = os.getcwd()\n",
    "# persist_directory_jd   = os.path.join(current_dir, \"chroma_db_jarvis_docs\")\n",
    "# os.makedirs(persist_directory_jd, exist_ok=True)\n",
    "# chroma_client_jd       = chromadb.PersistentClient(path=persist_directory_jd)\n",
    "\n",
    "# # drop & recreate collection \n",
    "# try:\n",
    "#     chroma_client_jd.delete_collection(\"jarvis_docs\")\n",
    "# except ValueError:\n",
    "#     pass\n",
    "\n",
    "# db_jd = Chroma(\n",
    "#     collection_name   = \"jarvis_docs\",\n",
    "#     embedding_function= embeddings,\n",
    "#     client            = chroma_client_jd,\n",
    "#     collection_metadata={\"hnsw:space\": \"cosine\"},\n",
    "# )\n",
    "\n",
    "# # ───────────────────────────  LOAD + INGEST ────────────────────────\n",
    "# s3_bucket  = \"plfs-han-llm-experiment\"\n",
    "# s3_folder  = 'factsheet-generation/Sidera/internal/jarvis_docs/'\n",
    "# ignored_files = []\n",
    "\n",
    "# texts = process_documents_from_s3(s3_bucket, s3_folder, ignored_files)\n",
    "\n",
    "# CHROMA_BATCH_SIZE = 10000\n",
    "# print(f\"Total documents to process: {len(texts)}  (batch {CHROMA_BATCH_SIZE})\")\n",
    "\n",
    "# for i in range(0, len(texts), CHROMA_BATCH_SIZE):\n",
    "#     batch_texts = texts[i : i + CHROMA_BATCH_SIZE]\n",
    "#     print(f\"Adding docs {i}-{i+len(batch_texts)-1}\")\n",
    "#     db_jd.add_documents(batch_texts)\n",
    "\n",
    "#     # housekeeping\n",
    "#     if DEVICE == \"cuda\":\n",
    "#         torch.cuda.empty_cache()\n",
    "#     gc.collect()\n",
    "\n",
    "# print(\"Vector database creation completed successfully!\")\n",
    "# print(\"Final document count:\", chroma_client_jd.get_collection(\"jarvis_docs\").count())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "319fd18b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## load saved vectordb\n",
    "# Init the same embedding function you used to build the DB\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "batch_size = 8 if DEVICE == \"cuda\" else 64\n",
    "\n",
    "embeddings = SentenceTransformerEmbeddings(\n",
    "    model_name=embeddings_model_name,\n",
    "    device=DEVICE,\n",
    "    batch_size=batch_size,\n",
    "    max_length=8192\n",
    ")\n",
    "\n",
    "\n",
    "persist_directory_jd = os.path.join(os.getcwd(), \"chroma_db_jarvis_docs\")\n",
    "chroma_client_jd   = chromadb.PersistentClient(path=persist_directory_jd)\n",
    "db_jd = Chroma(\n",
    "    client=chroma_client_jd,\n",
    "    collection_name=\"jarvis_docs\",\n",
    "    embedding_function=embeddings\n",
    ")\n",
    "\n",
    "\n",
    "# persist_directory_jt = os.path.join(os.getcwd(), \"chroma_db_jarvis_tables\")\n",
    "# chroma_client_jt   = chromadb.PersistentClient(path=persist_directory_jt)\n",
    "# db_jt = Chroma(\n",
    "#     client=chroma_client_jt,\n",
    "#     collection_name=\"jarvis_tables\",\n",
    "#     embedding_function=embeddings\n",
    "# )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c88cd1c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_page_contents_only(collection, batch_size=1000):\n",
    "    all_contents = []\n",
    "    offset = 0\n",
    "    \n",
    "    while True:\n",
    "        print(f\"Processing batch starting at offset {offset}...\")\n",
    "        \n",
    "        # Get only documents, no metadata or embeddings\n",
    "        batch_data = collection.get(\n",
    "            limit=batch_size,\n",
    "            offset=offset,\n",
    "            include=[\"documents\"]  # Only text content\n",
    "        )\n",
    "        \n",
    "        if not batch_data['documents']:\n",
    "            break\n",
    "            \n",
    "        all_contents.extend(batch_data['documents'])\n",
    "        offset += batch_size\n",
    "        print(f\"Processed {len(all_contents)} documents so far...\")\n",
    "    \n",
    "    return all_contents\n",
    "\n",
    "# Use the batched approach\n",
    "collection = chroma_client_jd.get_collection(\"jarvis_docs\")\n",
    "all_page_contents = get_page_contents_only(collection, batch_size=1000)  \n",
    "\n",
    "with open('all_page_contents_docs.pkl', 'wb') as f:\n",
    "    pickle.dump(all_page_contents, f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2005de79",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('all_page_contents_docs.pkl', 'rb') as f:\n",
    "    all_page_contents = pickle.load(f)\n",
    "\n",
    "all_docs = [Document(page_content=content) for content in all_page_contents]\n",
    "bm25_retriever = BM25Retriever.from_documents(all_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb8448f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_key(secret_name,region_name):\n",
    "\n",
    "    session = boto3.session.Session()\n",
    "    client = session.client(\n",
    "        service_name='secretsmanager',\n",
    "        region_name=region_name\n",
    "    )\n",
    "\n",
    "    try:\n",
    "        get_secret_value_response = client.get_secret_value(\n",
    "            SecretId=secret_name\n",
    "        )\n",
    "    except ClientError as e:\n",
    "        raise e\n",
    "\n",
    "    secret = get_secret_value_response['SecretString']\n",
    "    key = ast.literal_eval(secret)['key']\n",
    "    \n",
    "    return key\n",
    "\n",
    "openai_api_key=get_key(\"openai-api-key\", \"us-west-2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1668f6c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from openai import OpenAI\n",
    "\n",
    "# # initialize client\n",
    "# client = OpenAI(api_key = get_key(\"openai-api-key\",\"us-west-2\"))\n",
    "\n",
    "# # call the chat completions endpoint\n",
    "# response = client.chat.completions.create(\n",
    "#     model=\"gpt-4o-search-preview\",\n",
    "#     messages=[\n",
    "#         {\"role\": \"system\", \"content\": \"You are an expert in bioventure investing.\"},\n",
    "#         {\"role\": \"user\", \"content\": f\"what's the weather in Shanghai today.\"}\n",
    "#     ]\n",
    "# )\n",
    "\n",
    "# print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad48cd84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from openai import OpenAI\n",
    "# client = OpenAI(api_key = get_key(\"openai-api-key\",\"us-west-2\"))\n",
    "\n",
    "# response = client.responses.create(\n",
    "#     model=\"gpt-4.1\",\n",
    "#     tools=[{\n",
    "#         \"type\": \"web_search_preview\",\n",
    "#         \"search_context_size\": \"low\",\n",
    "#     }],\n",
    "#     input=\"What movie won best picture in 2025?\",\n",
    "# )\n",
    "\n",
    "# print(response.output_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "778fdde4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def answer_gpt(prompt):\n",
    "    \n",
    "#     from openai import OpenAI\n",
    "#     client = OpenAI(api_key = get_key(\"openai-api-key\",\"us-west-2\"))\n",
    "#     response = client.chat.completions.create(\n",
    "#                   model=\"gpt-4.1\", # or the latest version of GPT, o4-mini, gpt-4o, o3, gpt-4.1\n",
    "#                   temperature=0,\n",
    "#                   messages=[\n",
    "#                       {\"role\": \"system\", \"content\": \"You are an expert in bioventure investing.\"},\n",
    "#                       {\"role\": \"user\", \"content\": f\"answer the following question:{prompt}\"}\n",
    "#                     ]\n",
    "#                 )\n",
    "#     return response.choices[0].message.content\n",
    "\n",
    "\n",
    "def answer_gpt(prompt):\n",
    "    \n",
    "    from openai import OpenAI\n",
    "    client = OpenAI(api_key = get_key(\"openai-api-key\",\"us-west-2\"))\n",
    "    response = client.responses.create(\n",
    "                  model=\"gpt-4.1\", # or the latest version of GPT, o4-mini, gpt-4o, o3, gpt-4.1\n",
    "                  temperature=0,\n",
    "                  input=f\"You are an expert in bioventure investing. Answer the following question: {prompt}\"\n",
    "                )\n",
    "    return response.output_text\n",
    "\n",
    "\n",
    "# def answer_online_search(prompt):\n",
    "    \n",
    "#     from openai import OpenAI\n",
    "#     client = OpenAI(api_key = get_key(\"openai-api-key\",\"us-west-2\"))\n",
    "#     response = client.chat.completions.create(\n",
    "#                   model=\"gpt-4o-search-preview\",    #o4-mini, gpt-4o-search-preview\n",
    "#                   messages=[\n",
    "#                       {\"role\": \"system\", \"content\": \"You are an expert in bioventure investing.\"},\n",
    "#                       {\"role\": \"user\", \"content\": f\"{prompt}\"}\n",
    "#                     ]\n",
    "#                 )\n",
    "#     return response.choices[0].message.content\n",
    "\n",
    "\n",
    "def answer_online_search(prompt,search_model=\"o4-mini\"):\n",
    "    \n",
    "    from openai import OpenAI\n",
    "    client = OpenAI(api_key = get_key(\"openai-api-key\",\"us-west-2\"))\n",
    "    response = client.responses.create(\n",
    "                  model= search_model,    #o4-mini, o3\n",
    "                  tools=[{\"type\": \"web_search_preview\",\n",
    "                          \"search_context_size\": \"high\",}],\n",
    "                  input=f\"{prompt}\"                    \n",
    "                )\n",
    "    return response.output_text\n",
    "\n",
    "\n",
    "\n",
    "def origene_mcp(prompt, search_model=\"o4-mini\"):\n",
    "    \n",
    "    from openai import OpenAI\n",
    "    client = OpenAI(api_key = get_key(\"openai-api-key\",\"us-west-2\"))\n",
    "    response = client.responses.create(\n",
    "                model=search_model,\n",
    "                tools=[\n",
    "                    {\"type\": \"mcp\",\n",
    "                     \"server_label\": \"origene\",\n",
    "                     \"server_url\": \"https://origene-uuid1752754854.app-space.dplink.cc/chembl_mcp/mcp/?token=172f53102e0a46acb20f306eceaaf6c4\",\n",
    "                     \"require_approval\": \"never\",\n",
    "                    },],\n",
    "                input= f\"{prompt}\"\n",
    "                )\n",
    "    return response.output_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c8a5d94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ###take too long to run\n",
    "# from openai import OpenAI\n",
    "# client = OpenAI(api_key = get_key(\"openai-api-key\",\"us-west-2\"))\n",
    "# response = client.responses.create(\n",
    "#             model=\"o4-mini\",\n",
    "#             tools=[\n",
    "#                 {\"type\": \"mcp\",\n",
    "#                  \"server_label\": \"disease-pocket-molecule-mcp\",\n",
    "#                  \"server_url\": \"https://disease-pocket-molecule-mcp-uuid1751524197.app-space.dplink.cc/sse?token=4218554daa854930947d4986b1fb35e9\",\n",
    "#                  \"require_approval\": \"never\",\n",
    "#                 },],\n",
    "#             input= \"find all targets associated with obesity\"\n",
    "#             )\n",
    "# print(response.output_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70f88ff9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(answer_online_search(\"Comprehensively list all the biotech companies that are competitors to PPInnova (Peak Perform Innova).\",\"o4-mini\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c75df770",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(answer_online_search(\"Comprehensively list all the biotech companies with active programs targeting ALK7 for indication Obesity, T2D.\",\"o4-mini\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33fa1008",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# print(answer_online_search(\"tell me about Sidera Bio\",\"o3\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "144fd4c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(answer_online_search(\"For company Trevi Therapeutics, search google finance to provide the following\\\n",
    "#                                     information for the recent past 3, 6, 12 months: The highest and lowest stock prices,\\\n",
    "#                                     along with the corresponding dates. The largest single-day stock price change (either gain\\\n",
    "#                                     or loss) during that period, including the amount and the date it occurred.\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0ea14ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "def answer_perplexity_search(prompt):\n",
    "    \n",
    "    perplexity_api_key = get_key(\"perplexity-api-key\", \"us-west-2\")\n",
    "\n",
    "    # Build the payload for the Perplexity ai API.\n",
    "    payload = {\n",
    "        \"model\": \"sonar-reasoning-pro\",\n",
    "        \"messages\": [\n",
    "            {\"role\": \"system\", \"content\": \"\"\"You have extensive expertise in biotech investments.\\\n",
    "            Always format responses with clear '**Final Answer**' section at the end,\\\n",
    "            using bold markdown, with no additional commentary after it.\\ \n",
    "            Format responses WITHOUT ANY citations or reference numbers.\n",
    "            Provide only the final answer. It is important that you do not include any explanation on the steps below.\\\n",
    "            Do not show the intermediate steps information.\n",
    "            \"\"\"},\n",
    "            {\"role\": \"user\", \"content\": f\"answer the following question:{prompt}\"}\n",
    "        ],\n",
    "        \"max_tokens\": 8000,\n",
    "        \"temperature\": 0.2,\n",
    "        \"top_p\": 0.9,\n",
    "        \"search_domain_filter\": None,\n",
    "        \"return_images\": False,\n",
    "        \"return_related_questions\": False,\n",
    "        \"stream\": False,\n",
    "        \"response_format\": None\n",
    "    }\n",
    "    \n",
    "    headers = {\n",
    "        \"Authorization\": f\"Bearer {perplexity_api_key}\",\n",
    "        \"Content-Type\": \"application/json\"\n",
    "    }\n",
    "    \n",
    "    # Call the Perplexity ai API.\n",
    "    response = requests.post(\"https://api.perplexity.ai/chat/completions\", json=payload, headers=headers)\n",
    "    response.raise_for_status()  # Ensure that an HTTP error raises an exception.\n",
    "    \n",
    "    result_json = response.json()\n",
    "    \n",
    "    # Extract the answer from the API response.\n",
    "    full_response = result_json['choices'][0]['message']['content']\n",
    "    \n",
    "    # Split response into thinking process and final answer\n",
    "    if \"**Final Answer**\" in full_response:\n",
    "        answer = full_response.split(\"**Final Answer**\")[-1].strip()\n",
    "    else:  # Fallback if formatting changes\n",
    "        answer = full_response.split(\"\\n\\n\")[-1].strip()\n",
    "        \n",
    "    return answer    \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52767d94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ##for testing purpose\n",
    "# answer_perplexity_search(\"what's the weather in Shanghai today\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef627cff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def answer_with_search(question, db_jd, k_jd, priority_order=['perplexity', 'jarvis_docs']):\n",
    "        \n",
    "    # Retrieve documents from docs\n",
    "    jarvis_docs_docs = db_jd.similarity_search(question, k_jd)\n",
    "    \n",
    "#     bm25_retriever.k = 25\n",
    "#     vector_retriever = db_jd.as_retriever(search_kwargs={\"k\": k_jd})\n",
    "    \n",
    "#     ensemble = EnsembleRetriever(retrievers=[bm25_retriever, vector_retriever],\n",
    "#                                  weights=[0.5, 0.5])\n",
    "\n",
    "#     jarvis_docs_docs = ensemble.get_relevant_documents(question)\n",
    "    \n",
    "    # Get perplexity response if required\n",
    "    perplexity_response = answer_perplexity_search(question) if 'perplexity' in priority_order else \"\"\n",
    "    \n",
    "    # Create source-to-context mapping\n",
    "    source_contexts = {\n",
    "        'jarvis_docs': [d.page_content for d in jarvis_docs_docs],\n",
    "        'perplexity': [perplexity_response] if perplexity_response else []\n",
    "    }    \n",
    "    \n",
    "#     combined_contexts = []\n",
    "#     for source in priority_order:\n",
    "#         if source in source_contexts:\n",
    "#             combined_contexts += (source_contexts[source])\n",
    "    \n",
    "    # Build the knowledge base from each source\n",
    "    knowledge_base = {\n",
    "        'jarvis_docs': \"\\n\\n\".join(d.page_content for d in jarvis_docs_docs) if 'jarvis_docs' in priority_order else \"\",\n",
    "        'perplexity': perplexity_response if 'perplexity' in priority_order else \"\"\n",
    "    }\n",
    "    \n",
    "    # Build prioritized context using the given priority order.\n",
    "    priority_context = []\n",
    "    for idx, source in enumerate(priority_order, 1):\n",
    "        heading = {\n",
    "            'jarvis_docs': f\"{idx}. JARVIS Docs\",\n",
    "            'perplexity': f\"{idx}. External Search\"\n",
    "        }[source]\n",
    "        \n",
    "        content = knowledge_base[source] or f\"No {source} data available\"\n",
    "        priority_context.append(f\"{heading}:\\n{content}\")\n",
    "    \n",
    "    # Generate source counts for jarvis_tables and jarvis_docs\n",
    "    source_counts = {\n",
    "        'jarvis_docs': Counter(os.path.basename(doc.metadata['source']) for doc in jarvis_docs_docs) if knowledge_base['jarvis_docs'] else Counter(),\n",
    "        'external_sources': \"perplexity:1\" if knowledge_base['perplexity'] else \"perplexity:0\"\n",
    "    }\n",
    "    \n",
    "    overview_images = list({\n",
    "        m['overview_image'] for m in (doc.metadata for doc in jarvis_docs_docs) if 'overview_image' in m\n",
    "    })\n",
    "    \n",
    "    # Precompute the joined priority context to avoid issues with backslashes in f-string expressions.\n",
    "    joined_priority_context = \"\\n\\n\".join(priority_context)\n",
    "    \n",
    "    prompt = f\"\"\"\n",
    "**Analysis Directive**: Answer using this priority sequence: {', '.join(priority_order).upper()}\n",
    "\n",
    "**Knowledge Base**:\n",
    "{joined_priority_context}\n",
    "\n",
    "**Conflict Resolution Rules**:\n",
    "- Follow {priority_order[0].upper()} for numerical disputes\n",
    "- Resolve conceptual conflicts using {priority_order[0].upper()}\n",
    "- Use most recent context when dates conflict\n",
    "\n",
    "**Question**: {question}\n",
    "\n",
    "**Response Requirements**:\n",
    "Do not fabricate any information that is not in the given content.\n",
    "Answer in formal written English. Please provide a response with a concise introductory phrase,\n",
    "but avoid meaningless fillers like 'ok', 'sure' or 'certainly'. Focus on delivering a direct and informative answer.\n",
    "Do not include reference filenames in the answer.\n",
    "\"\"\"\n",
    "    \n",
    "    return answer_gpt(prompt), source_counts, overview_images, perplexity_response    #combined_contexts\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17c0b21b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def answer_with_search_ensemble(question, bm25_retriever, k_bm, db_jd, k_jd, priority_order=['perplexity', 'jarvis_docs']):\n",
    "        \n",
    "#     # Retrieve documents from docs    \n",
    "#     bm25_retriever.k = k_bm\n",
    "#     vector_retriever = db_jd.as_retriever(search_kwargs={\"k\": k_jd})\n",
    "    \n",
    "#     ensemble = EnsembleRetriever(retrievers=[bm25_retriever, vector_retriever],\n",
    "#                                  weights=[0.5, 0.5])\n",
    "\n",
    "#     jarvis_docs_docs = ensemble.get_relevant_documents(question)\n",
    "    \n",
    "#     # Get perplexity response if required\n",
    "#     perplexity_response = answer_perplexity_search(question) if 'perplexity' in priority_order else \"\"\n",
    "    \n",
    "#     # Create source-to-context mapping\n",
    "#     source_contexts = {\n",
    "#         'jarvis_docs': [d.page_content for d in jarvis_docs_docs],\n",
    "#         'perplexity': [perplexity_response] if perplexity_response else []\n",
    "#     }    \n",
    "    \n",
    "# #     combined_contexts = []\n",
    "# #     for source in priority_order:\n",
    "# #         if source in source_contexts:\n",
    "# #             combined_contexts += (source_contexts[source])\n",
    "    \n",
    "#     # Build the knowledge base from each source\n",
    "#     knowledge_base = {\n",
    "#         'jarvis_docs': \"\\n\\n\".join(d.page_content for d in jarvis_docs_docs) if 'jarvis_docs' in priority_order else \"\",\n",
    "#         'perplexity': perplexity_response if 'perplexity' in priority_order else \"\"\n",
    "#     }\n",
    "    \n",
    "#     # Build prioritized context using the given priority order.\n",
    "#     priority_context = []\n",
    "#     for idx, source in enumerate(priority_order, 1):\n",
    "#         heading = {\n",
    "#             'jarvis_docs': f\"{idx}. JARVIS Docs\",\n",
    "#             'perplexity': f\"{idx}. External Search\"\n",
    "#         }[source]\n",
    "        \n",
    "#         content = knowledge_base[source] or f\"No {source} data available\"\n",
    "#         priority_context.append(f\"{heading}:\\n{content}\")\n",
    "    \n",
    "#     # Generate source counts for jarvis_tables and jarvis_docs\n",
    "# #     source_counts = {\n",
    "# #         'jarvis_tables': Counter(os.path.basename(doc.metadata['source']) for doc in jarvis_tables_docs) if knowledge_base['jarvis_tables'] else Counter(),\n",
    "# #         'jarvis_docs': Counter(os.path.basename(doc.metadata['source']) for doc in jarvis_docs_docs) if knowledge_base['jarvis_docs'] else Counter(),\n",
    "# #         'external_sources': \"perplexity:1\" if knowledge_base['perplexity'] else \"perplexity:0\"\n",
    "# #     }\n",
    "    \n",
    "# #     overview_images = list({\n",
    "# #         m['overview_image'] for m in (doc.metadata for doc in jarvis_docs_docs) if 'overview_image' in m\n",
    "# #     })\n",
    "    \n",
    "#     # Precompute the joined priority context to avoid issues with backslashes in f-string expressions.\n",
    "#     joined_priority_context = \"\\n\\n\".join(priority_context)\n",
    "    \n",
    "#     prompt = f\"\"\"\n",
    "# **Analysis Directive**: Answer using this priority sequence: {', '.join(priority_order).upper()}\n",
    "\n",
    "# **Knowledge Base**:\n",
    "# {joined_priority_context}\n",
    "\n",
    "# **Conflict Resolution Rules**:\n",
    "# - Follow {priority_order[0].upper()} for numerical disputes\n",
    "# - Resolve conceptual conflicts using {priority_order[0].upper()}\n",
    "# - Use most recent context when dates conflict\n",
    "\n",
    "# **Question**: {question}\n",
    "\n",
    "# **Response Requirements**:\n",
    "# Do not fabricate any information that is not in the given content.\n",
    "# Answer in formal written English. Please provide a response with a concise introductory phrase,\n",
    "# but avoid meaningless fillers like 'ok', 'sure' or 'certainly'. Focus on delivering a direct and informative answer.\n",
    "# Do not include reference filenames in the answer.\n",
    "# \"\"\"\n",
    "    \n",
    "#     return answer_gpt(prompt), perplexity_response    #combined_contexts\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c340181",
   "metadata": {},
   "outputs": [],
   "source": [
    "def answer_with_search_ensemble(question, bm25_retriever, k_bm, db_jd, k_jd, search_model=\"gpt-4.1\", priority_order=['online_search', 'jarvis_docs']):\n",
    "        \n",
    "    # Retrieve documents from docs    \n",
    "    bm25_retriever.k = k_bm\n",
    "    vector_retriever = db_jd.as_retriever(search_kwargs={\"k\": k_jd})\n",
    "    \n",
    "    ensemble = EnsembleRetriever(retrievers=[bm25_retriever, vector_retriever],\n",
    "                                 weights=[0.5, 0.5])\n",
    "\n",
    "    jarvis_docs_docs = ensemble.get_relevant_documents(question)\n",
    "    \n",
    "    # Get online_search response if required\n",
    "    online_search_response = answer_online_search(question, search_model) if 'online_search' in priority_order else \"\"\n",
    "    \n",
    "    # Create source-to-context mapping\n",
    "    source_contexts = {\n",
    "        'jarvis_docs': [d.page_content for d in jarvis_docs_docs],\n",
    "        'online_search': [online_search_response] if online_search_response else []\n",
    "    }    \n",
    "    \n",
    "#     combined_contexts = []\n",
    "#     for source in priority_order:\n",
    "#         if source in source_contexts:\n",
    "#             combined_contexts += (source_contexts[source])\n",
    "    \n",
    "    # Build the knowledge base from each source\n",
    "    knowledge_base = {\n",
    "        'jarvis_docs': \"\\n\\n\".join(d.page_content for d in jarvis_docs_docs) if 'jarvis_docs' in priority_order else \"\",\n",
    "        'online_search': online_search_response if 'online_search' in priority_order else \"\"\n",
    "    }\n",
    "    \n",
    "    # Build prioritized context using the given priority order.\n",
    "    priority_context = []\n",
    "    for idx, source in enumerate(priority_order, 1):\n",
    "        heading = {\n",
    "            'jarvis_docs': f\"{idx}. JARVIS Docs\",\n",
    "            'online_search': f\"{idx}. External Search\"\n",
    "        }[source]\n",
    "        \n",
    "        content = knowledge_base[source] or f\"No {source} data available\"\n",
    "        priority_context.append(f\"{heading}:\\n{content}\")\n",
    "    \n",
    "    # Generate source counts for jarvis_tables and jarvis_docs\n",
    "#     source_counts = {\n",
    "#         'jarvis_tables': Counter(os.path.basename(doc.metadata['source']) for doc in jarvis_tables_docs) if knowledge_base['jarvis_tables'] else Counter(),\n",
    "#         'jarvis_docs': Counter(os.path.basename(doc.metadata['source']) for doc in jarvis_docs_docs) if knowledge_base['jarvis_docs'] else Counter(),\n",
    "#         'external_sources': \"online_search:1\" if knowledge_base['online_search'] else \"online_search:0\"\n",
    "#     }\n",
    "    \n",
    "#     overview_images = list({\n",
    "#         m['overview_image'] for m in (doc.metadata for doc in jarvis_docs_docs) if 'overview_image' in m\n",
    "#     })\n",
    "    \n",
    "    # Precompute the joined priority context to avoid issues with backslashes in f-string expressions.\n",
    "    joined_priority_context = \"\\n\\n\".join(priority_context)\n",
    "    \n",
    "    prompt = f\"\"\"\n",
    "**Analysis Directive**: Answer using this priority sequence: {', '.join(priority_order).upper()}\n",
    "\n",
    "**Knowledge Base**:\n",
    "{joined_priority_context}\n",
    "\n",
    "**Conflict Resolution Rules**:\n",
    "- Follow {priority_order[0].upper()} for numerical disputes\n",
    "- Resolve conceptual conflicts using {priority_order[0].upper()}\n",
    "- Use most recent context when dates conflict\n",
    "\n",
    "**Question**: {question}\n",
    "\n",
    "**Response Requirements**:\n",
    "Do not fabricate any information that is not in the given content.\n",
    "Answer in formal written English, be objectively and factually, avoid subjective adjectives or exaggerations.\\\n",
    "Please provide a response with a concise introductory phrase,\n",
    "but avoid meaningless fillers like 'ok', 'sure' or 'certainly'. Focus on delivering a direct and informative answer.\n",
    "Please bold the most important facts or conclusions in your answer to help readers quickly identify key information,\\\n",
    "especially when the response is long.\n",
    "Do not include reference filenames in the answer.\n",
    "\"\"\"\n",
    "    \n",
    "    return answer_gpt(prompt), online_search_response    #combined_contexts\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a96cc630",
   "metadata": {},
   "outputs": [],
   "source": [
    "# question1=f\"\"\"Which financing round (e.g. seed, series A, series B, etc) of company {COMPANY_NAME} is currently in.\"\"\"\n",
    "# result, source_counts, overview_images = answer_with_image_old(question1, db, 50)\n",
    "# print(result, f\"\\n\\n{source_counts}\", f\"\\n\\nnum of images: {len(overview_images)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d550292c",
   "metadata": {},
   "outputs": [],
   "source": [
    "###test\n",
    "def answer_perplexity_search_test(prompt):\n",
    "    \n",
    "    perplexity_api_key = get_key(\"perplexity-api-key\", \"us-west-2\")\n",
    "\n",
    "    # Build the payload for the Perplexity ai API.\n",
    "    payload = {\n",
    "        \"model\": \"sonar-reasoning-pro\",\n",
    "        \"messages\": [\n",
    "            {\"role\": \"system\", \"content\": \"\"\"You have extensive expertise in biotech investments.\\\n",
    "            At the end of your answer, make sure to include all the online sources' full URLs you used for your think process.\n",
    "            \"\"\"},\n",
    "            {\"role\": \"user\", \"content\": f\"answer the following question:{prompt}\"}\n",
    "        ],\n",
    "        \"max_tokens\": 8000,\n",
    "        \"temperature\": 0.2,\n",
    "        \"top_p\": 0.9,\n",
    "        \"search_domain_filter\": None,\n",
    "        \"return_images\": False,\n",
    "        \"return_related_questions\": False,\n",
    "        \"stream\": False,\n",
    "        \"response_format\": None\n",
    "    }\n",
    "    \n",
    "    headers = {\n",
    "        \"Authorization\": f\"Bearer {perplexity_api_key}\",\n",
    "        \"Content-Type\": \"application/json\"\n",
    "    }\n",
    "    \n",
    "    # Call the Perplexity ai API.\n",
    "    response = requests.post(\"https://api.perplexity.ai/chat/completions\", json=payload, headers=headers)\n",
    "    response.raise_for_status()  # Ensure that an HTTP error raises an exception.\n",
    "    \n",
    "    result_json = response.json()\n",
    "    \n",
    "    # Extract the answer from the API response.\n",
    "    full_response = result_json['choices'][0]['message']['content']\n",
    "    \n",
    "#     # Split response into thinking process and final answer\n",
    "#     if \"**Final Answer**\" in full_response:\n",
    "#         answer = full_response.split(\"**Final Answer**\")[-1].strip()\n",
    "#     else:  # Fallback if formatting changes\n",
    "#         answer = full_response.split(\"\\n\\n\")[-1].strip()\n",
    "        \n",
    "    return full_response    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce9e01f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def company_helper(company_name):\n",
    "    \n",
    "    question = f\"\"\"\n",
    "    Extract structured facts about the drug pipelines of company {COMPANY_NAME}.\n",
    "    Return ONLY a JSON object with company-level information and its key assets.\n",
    "    Include only assets with a known asset name.\n",
    "    If no assets are found, return an empty assets list.\n",
    "\n",
    "    JSON structure:\n",
    "    {{\n",
    "      \"company name\": \"{COMPANY_NAME}\",\n",
    "      \"has platform\": true | false | null,\n",
    "      \"platform name\": \"<name, else null>\",\n",
    "      \"platform is core asset\": true | false | null,\n",
    "      \"assets\": [\n",
    "        {{\n",
    "          \"asset name\": \"<name, else null>\",\n",
    "          \"modality\": \"<name, else null>\",\n",
    "          \"targets\": [\"...\"],\n",
    "          \"targeted therapeutic areas\": [\"...\"],  \n",
    "          \"targeted indications\": [\"...\"],\n",
    "          \"current development stage\": \"<name, else null>\", \n",
    "          \"brief trial result\": \"<brief description, else null>\",\n",
    "          \"companies with competing asset\":[\"...\"],\n",
    "        }}\n",
    "      ]\n",
    "    }}\n",
    "    \"\"\"\n",
    "\n",
    "    result, online_search_response = answer_with_search_ensemble(question, bm25_retriever, 100, db_jd, 100, search_model=\"o4-mini\", priority_order=[\"jarvis_docs\"])\n",
    "    \n",
    "    return result\n",
    "\n",
    "    \n",
    "\n",
    "def prompt_format(json_string):\n",
    "    \n",
    "    from openai import OpenAI\n",
    "    client = OpenAI(api_key=get_key(\"openai-api-key\", \"us-west-2\"))\n",
    "\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"gpt-4.1\",  # Use the latest GPT-4 model you have access to\n",
    "        temperature=0,\n",
    "        messages=[\n",
    "            {\n",
    "                \"role\": \"system\",\n",
    "                \"content\": (\"You are an expert in json structure.\")\n",
    "            },\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": f\"\"\"I want you to review the json string and make sure it's properly formatted.\\\n",
    "                Return the correct formatted json string of {json_string} as the output.\\\n",
    "                Please only return the json string, do not add any introductory phrase.\"\"\"\n",
    "            }\n",
    "        ]\n",
    "    )\n",
    "    \n",
    "    response_string=response.choices[0].message.content\n",
    "    json_match = re.search(r'{.*}', response_string, re.DOTALL)\n",
    "    if json_match:\n",
    "        json_content = json_match.group()\n",
    "        # Parse the JSON content to ensure it is valid\n",
    "        try:\n",
    "            parsed_json = json.loads(json_content)\n",
    "            return parsed_json\n",
    "        except json.JSONDecodeError as e:\n",
    "            print(f\"Error decoding JSON: {e}\")\n",
    "            return None\n",
    "    else:\n",
    "        print(\"No JSON object found in the input string.\")\n",
    "        return None    \n",
    "\n",
    "\n",
    "def company_extractor(COMPANY_NAME):\n",
    "    \n",
    "    info = company_helper(COMPANY_NAME)\n",
    "    format_info = prompt_format(info)\n",
    "    \n",
    "    return format_info\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ae1f690",
   "metadata": {},
   "outputs": [],
   "source": [
    "# COMPANY_NAME = 'Euhearing Therapeutics'\n",
    "# question1=f\"\"\"\n",
    "#     Extract structured facts about the drug pipelines of company {COMPANY_NAME}.\n",
    "#     Return ONLY a JSON object with company-level information and its key assets.\n",
    "#     Include only assets with a known asset name.\n",
    "#     If no assets are found, return an empty assets list.\n",
    "\n",
    "#     JSON structure:\n",
    "#     {{\n",
    "#       \"company name\": \"{COMPANY_NAME}\",\n",
    "#       \"has platform\": true | false | null,\n",
    "#       \"platform name\": \"<name, else null>\",\n",
    "#       \"platform is core asset\": true | false | null,\n",
    "#       \"assets\": [\n",
    "#         {{\n",
    "#           \"asset name\": \"<name, else null>\",\n",
    "#           \"modality\": \"<name, else null>\",\n",
    "#           \"targets\": [\"...\"],\n",
    "#           \"targeted therapeutic areas\": [\"...\"],  \n",
    "#           \"targeted indications\": [\"...\"],\n",
    "#           \"current development stage\": \"<name, else null>\", \n",
    "#           \"brief trial result\": \"<brief description, else null>\",\n",
    "#           \"companies with competing asset\":[\"...\"],\n",
    "#         }}\n",
    "#       ]\n",
    "#     }}\n",
    "#     \"\"\"\n",
    "# bm25_retriever.k = 50\n",
    "# vector_retriever = db_jd.as_retriever(search_kwargs={\"k\": 100})\n",
    "\n",
    "# ensemble = EnsembleRetriever(retrievers=[bm25_retriever, vector_retriever],\n",
    "#                              weights=[0.5, 0.5])\n",
    "\n",
    "# jarvis_docs_docs = ensemble.get_relevant_documents(question1)\n",
    "# [d.page_content for d in jarvis_docs_docs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57226b3c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "COMPANY_NAME = \"PPInnova (Peak Perform Innova)\"\n",
    "company_info = company_extractor(COMPANY_NAME)\n",
    "company_info    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "723a0b0f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "### human review to update some info\n",
    "company_info[\"platform is core asset\"] = True\n",
    "substrings = ['STAT6', 'RBM39']\n",
    "company_info['assets'] = [asset for key in substrings for asset in company_info['assets'] if key in asset['asset name']]\n",
    "for asset in company_info[\"assets\"]:\n",
    "#     if \"EHT102\" in asset[\"asset name\"]:\n",
    "#         to_remove = [\"RRGENE\", \"Fudan University\"]\n",
    "#         to_add = [\"HuidaGene Therapeutics\",]\n",
    "#         asset[\"companies with competing asset\"] = [s for s in asset[\"companies with competing asset\"] if not any(sub in s for sub in to_remove)]\n",
    "    \n",
    "    asset['competitor_valuation'] = []   \n",
    "    for company in asset['companies with competing asset']:\n",
    "        print(f\"Getting valuation for {company}...\")        \n",
    "        query = f\"\"\"If {company} is a private company, provide its latest post-money valuation.\n",
    "                    If {company} was acquired, provide the acquisition deal size.\n",
    "                    If {company} is public, fetch its latest market cap from Google Finance.\"\"\"\n",
    "        result = answer_online_search(query, \"o4-mini\")\n",
    "        asset['competitor_valuation'].append(result)\n",
    "        time.sleep(1)\n",
    "        \n",
    "# for asset in company_info[\"assets\"]:\n",
    "#     if \"EHT102\" in asset[\"asset name\"]:\n",
    "#         asset[\"targeted indications\"] = [\"Obesity\",\"T2D\"]\n",
    "#     if \"FGF21\" in asset[\"asset name\"]:\n",
    "#         asset[\"targeted indications\"] = ['Chronic Kidney Disease (CKD)', 'Diabetic Kidney Disease (DKD)', 'Metabolic dysfunction-associated steatohepatitis (MASH)']\n",
    "        \n",
    "company_info        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "917109f1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# ### human review to update some info\n",
    "# company_info[\"platform is core asset\"] = False\n",
    "# company_info['assets'] = [asset for asset in company_info['assets'] if asset['asset name'] in ['EHT102','EHT201']]\n",
    "\n",
    "# for asset in company_info[\"assets\"]:\n",
    "#     if \"EHT102\" in asset[\"asset name\"]:\n",
    "#         to_remove = [\"RRGENE\", \"Fudan University\"]\n",
    "#         to_add = [\"Emaygene\"]\n",
    "#         asset[\"companies with competing asset\"] = [s for s in asset[\"companies with competing asset\"] if not any(sub in s for sub in to_remove)]\n",
    "#         asset[\"companies with competing asset\"].extend(to_add)\n",
    "    \n",
    "#     if \"EHT201\" in asset[\"asset name\"]:\n",
    "#         to_add = [\"Decibel Therapeutics\",\n",
    "#                   \"Otonomy & AGTC (OTO-825)\",\n",
    "#                   \"Harvard Medical School - David Corey\",\n",
    "#                   \"Southeast University (Chai Renjie) Program\",\n",
    "#                   \"Juntendo University - Kazusaku Kamiya\"\n",
    "#                  ]\n",
    "#         asset[\"companies with competing asset\"].extend(to_add)\n",
    "    \n",
    "#     # Get competitor valuations for all assets\n",
    "#     asset['competitor_valuation'] = []\n",
    "#     for company in asset['companies with competing asset']:\n",
    "#         print(f\"Getting valuation for {company}...\")\n",
    "#         query = f\"\"\"If {company} is a private company, provide its latest post-money valuation.\n",
    "#                     If {company} was acquired, provide the acquisition deal size.\n",
    "#                     If {company} is public, fetch its latest market cap from Google Finance.\"\"\"\n",
    "#         result = answer_online_search(query, \"o4-mini\")\n",
    "#         asset['competitor_valuation'].append(result)\n",
    "#         time.sleep(1)\n",
    "\n",
    "# company_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cdfacf1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "COMPANY_NAME = \"Sidera Bio\"\n",
    "\n",
    "question1=f\"\"\"\n",
    "Outline the deal terms associated with company {COMPANY_NAME}'s current financing round.\n",
    "\"\"\"\n",
    "result, online_search_response = answer_with_search_ensemble(question1, bm25_retriever, 25, db_jd, 25, search_model=\"o4-mini\", priority_order=[\"jarvis_docs\"])\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00dd1a7d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f22f9e3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "COMPANY_NAME = \"Euhearing Therapeutics\"\n",
    "\n",
    "question1=f\"\"\"\n",
    "For company Euhearing Therapeutics's asset EHT102's targeted indication OTOF-related deafness, DFNB9 congenital hearing loss,\n",
    "estimate the diagnosed and\n",
    "total patient populations in the USA, Europe, and globally. \n",
    "Based on incidence and prevalence rates,\n",
    "note whether patient numbers are growing or declining, and define the therapy-eligible population\n",
    "considering line of therapy, disease stage, or biomarker subgroups. Estimate the global total\n",
    "addressable market (in U.S. dollars). \n",
    "Propose an annual price relative to the standard of care,                    \n",
    "estimate peak sales at a reasonable market-share penetration.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "result, online_search_response = answer_with_search_ensemble(question1, bm25_retriever, 50, db_jd, 50, search_model=\"o4-mini\", priority_order=[\"jarvis_docs\"])\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c031a242",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "COMPANY_NAME = \"Euhearing Therapeutics\"\n",
    "\n",
    "question1=f\"\"\"\n",
    "Describe the clinical trial process and result of asset EHT101 in company Euhearing Therapeutics.\n",
    "\"\"\"\n",
    "\n",
    "result, online_search_response = answer_with_search_ensemble(question1, bm25_retriever, 50, db_jd, 50, search_model=\"o3\", priority_order=[\"jarvis_docs\"])\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "253af7d8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "question1=f\"\"\"\n",
    "For company Euhearing Therapeutics's asset EHT102's targeted indication OTOF-related deafness, DFNB9 congenital hearing loss,\n",
    "estimate the diagnosed and prevalent patient populations in China, and the population asset EHT102 can target.\n",
    "Show your estimation process step by step.\n",
    "\"\"\"\n",
    "\n",
    "result, online_search_response = answer_with_search_ensemble(question1, bm25_retriever, 50, db_jd, 50, search_model=\"o3\", priority_order=[\"online_search\"])\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1634fff0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "question1=f\"\"\"\n",
    "For asset EHT102's targeted indication OTOF-related deafness, DFNB9 congenital hearing loss,\n",
    "estimate the diagnosed and prevalent patient populations in China, and the population asset EHT102 can target.\n",
    "Show your estimation process step by step.\n",
    "\"\"\"\n",
    "\n",
    "result, online_search_response = answer_with_search_ensemble(question1, bm25_retriever, 50, db_jd, 50, search_model=\"o4-mini\", priority_order=[\"online_search\"])\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bbddd5b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36c72d0f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "COMPANY_NAME = \"Euhearing\"\n",
    "\n",
    "question1=f\"\"\"\n",
    "For asset EHT102's targeted indication OTOF-related deafness, DFNB9, estimate the diagnosed and\n",
    "prevalent patient populations in China, and the population asset EHT102 can target in China.\n",
    "Validate your data assumption with latest reports or literatures.\n",
    "Show your estimation process step by step and list the reference links you used.\"\"\"\n",
    "result, online_search_response = answer_with_search_ensemble(question1, bm25_retriever, 50, db_jd, 50, search_model=\"o4-mini\", priority_order=[\"online_search\"])\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5b795af",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "company_name = \"Euhearing Therapeutics\"\n",
    "\n",
    "question1=f\"\"\"\n",
    "For asset EHT102, please provide a well‑organized response of its\n",
    "Competitive Context: comprehensively list all biotech companies with active programs targeting OTOF\n",
    "for indication OTOF-related deafness, DFNB9, such as competitors Decibel Therapeutics (DB-OTO),\n",
    "Akouos (AK-OTOF), Sensorion/Pasteur Institute (SENS-501/OTOF-GT), Otovia Therapeutics (OTOV101N+OTOV101C),\n",
    "HuidaGene Therapeutics (AAV-gOTOF-emxABE), EmayGene (EA0010), Katholieke Universiteit Leuven (WO2025003513A1).\n",
    "For each competitor, include: program, modality, clinical phase/status, key distinguishing features,\n",
    "and financial information according to the following rules:        \n",
    "- If the competitor is a private company, provide its latest post-money valuation.\n",
    "- If the competitor was acquired, provide the acquisition deal size.\n",
    "- If the competitor is public, fetch its **current** market cap using Google Finance and include the Google Finance URL. \n",
    "Do not use any other data sources for market cap.\n",
    "Present the information in a structured table, followed by a concise descriptive summary.\"\"\"\n",
    "result, online_search_response = answer_with_search_ensemble(question1, bm25_retriever, 50, db_jd, 50, search_model=\"o4-mini\", priority_order=[\"jarvis_docs\",\"online_search\"])\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dd92979",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(online_search_response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d2ced21",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "COMPANY_NAME = \"Euhearing Therapeutics\"\n",
    "\n",
    "question1=f\"\"\"\n",
    "Asset EHT102 is targeting OTOF-related deafness via Gene therapy (dual-AAV, protein-level recombination via intein).\n",
    "For asset EHT102's targeted indication OTOF-related deafness, estimate the diagnosed and\n",
    "prevalent patient populations in China, and the population asset EHT102 can target in China.\\\n",
    "Validate your data assumption with latest reports or literatures. \n",
    "Show your estimation process step by step and list the reference links you used.\n",
    "\"\"\"\n",
    "result, online_search_response = answer_with_search_ensemble(question1, bm25_retriever, 50, db_jd, 50, search_model=\"o4-mini\", priority_order=[\"online_search\"])\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fea97613",
   "metadata": {},
   "outputs": [],
   "source": [
    "COMPANY_NAME = \"Euhearing Therapeutics\"\n",
    "\n",
    "question1=f\"\"\"\n",
    "Asset EHT102 is targeting OTOF-related deafness via Gene therapy (dual-AAV, protein-level recombination via intein).\n",
    "Consider the price of current SoC for OTOF-related deafness and price of gene therapy for other rare diseases in China,\\\n",
    "come up an reasonable price for EHT102 in China.\\\n",
    "Validate your data assumption with latest reports or literatures. \n",
    "Show your estimation process step by step and list the reference links you used.\n",
    "\"\"\"\n",
    "result, online_search_response = answer_with_search_ensemble(question1, bm25_retriever, 50, db_jd, 50, search_model=\"o4-mini\", priority_order=[\"online_search\"])\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ab9f02d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "company_name = \"Euhearing Therapeutics\"\n",
    "\n",
    "question1=f\"\"\"\n",
    "Briefly state how we (Pivotal BioVenture Partners) learn about the company {company_name}.\\\n",
    "                            Then describe the previous financing history of company {company_name} prior to\\\n",
    "                            the current round. What rounds of funding has the company previously completed (size,\\\n",
    "                            investors etc.), and what were the key milestones achieved with each round? For company\\\n",
    "                            {company_name}'s most recent prior financing round, how much was raised and what were\\\n",
    "                            the pre-money and post-money valuation of the round?\\\n",
    "                            Please provide a single, cohesive paragraph to address all the questions.\n",
    "\"\"\"\n",
    "result, online_search_response = answer_with_search_ensemble(question1, bm25_retriever, 50, db_jd, 50, search_model=\"o4-mini\", priority_order=[\"jarvis_docs\"])\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43051c25",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(online_search_response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97fb8ffa",
   "metadata": {},
   "outputs": [],
   "source": [
    "documents=db_jd.similarity_search_with_relevance_scores(question1, 50)   #similarity_search, similarity_search_with_score, similarity_search_with_relevance_scores\n",
    "for chunk, score in documents:\n",
    "    print(chunk.metadata['source'])\n",
    "    print(score)\n",
    "    print(chunk.page_content)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d0afd97",
   "metadata": {},
   "outputs": [],
   "source": [
    "bm25_retriever.k = 50\n",
    "vector_retriever = db_jd.as_retriever(search_kwargs={\"k\": 50})\n",
    "\n",
    "ensemble = EnsembleRetriever(retrievers=[bm25_retriever, vector_retriever],\n",
    "                             weights=[0.5, 0.5])\n",
    "\n",
    "jarvis_docs_docs = ensemble.get_relevant_documents(question1)\n",
    "[d.page_content for d in jarvis_docs_docs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b4117d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "COMPANY_NAME = \"Euhearing Therapeutics\"\n",
    "\n",
    "question1=f\"\"\"\n",
    "The asset EHT102 in company Euhearing Therapeutics's pipeline is in development stage Phase 1/2 (IND filed, first injection Dec 2024).                            If the asset EHT102 is in clinical stage, depending on the trial phase, provide the following:                            If the asset EHT102 is in Phase 1, describe the study                            design (including randomization, blinding, and control arms if used), participant type and number                            (healthy volunteers or patients), primary endpoints, and summarize quantitative results                            for safety, dose-limiting toxicities, and tolerability; additionally, provide any                            exploratory changes of biomarker data or preliminary signals of efficacy with concrete numberic results.                             If the asset EHT102 is in Phase 2, detail the study design (e.g., dose-finding, proof-of-concept), patient                            population and inclusion/exclusion criteria, primary and secondary endpoints (often initial                            efficacy and extended safety), and present efficacy and safety results using concrete quantitative                            metrics (response rates, changes in relevant biomarkers, mean differences, etc.), p-values, and                            interpret whether the results support advancement to pivotal trials.                            If the asset EHT102 is in Phase 3, provide a comprehensive summary of the pivotal study design (randomization, control arms,                            multicenter participation), number of patients, detailed definitions of all primary and secondary                            endpoints, and full clinical results, including efficacy, safety, effect sizes, risk reductions,                            confidence intervals, and p-values, clearly assessing if the trial met its primary objectives and                            is likely to support regulatory approval or label expansion.                            For the trial, specify if the outcome was positive, negative, or inconclusive based on primary endpoints,                            and interpret the significance of the findings in light of current clinical standards                            and patient population needs. Note any missing data or information gaps.                            If the asset EHT102 is not in clinical stage, simply state that it's not in clinical stage.\"\"\"\n",
    "result, online_search_response = answer_with_search_ensemble(question1, bm25_retriever, 50, db_jd, 50, search_model=\"o4-mini\", priority_order=[\"jarvis_docs\"])\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d453dda",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fc620b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "documents=db_jt.similarity_search_with_relevance_scores(question1, 100)   #similarity_search, similarity_search_with_score, similarity_search_with_relevance_scores\n",
    "for chunk, score in documents:\n",
    "    print(chunk.metadata['source'])\n",
    "    print(score)\n",
    "    print(chunk.page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "334e4043",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_questions_with_assets(company_name, company_info):\n",
    "    \"\"\"\n",
    "    Generate the full questions JSON with dynamic asset-specific competitive landscape questions.\n",
    "    \n",
    "    Args:\n",
    "        company_name: Name of the company\n",
    "        company_extractor: Function to extract company data\n",
    "    \n",
    "    Returns:\n",
    "        JSON string with questions\n",
    "    \"\"\"\n",
    "    # Generate mechanism and pre-clinical questions for each asset    \n",
    "    mechanism_questions = []\n",
    "    for asset in company_info['assets']:\n",
    "        asset_name = asset['asset name']\n",
    "        targets = ', '.join(asset['targets'])\n",
    "        indications = ', '.join(asset['targeted indications'])\n",
    "        mechanism_question_text = f\"\"\"For the asset {asset_name} in company {company_name}'s pipeline, please address the following:\n",
    "                    1. Mechanism-of-Action Evidence: Summarize the data supporting its mechanism of action in the\\\n",
    "                    targeted indication (e.g., biochemical assays, cell-based studies, animal models or clinical\\\n",
    "                    results); If available, describe any genetic or biomarker evidence linking the target {targets} or mechanism\\\n",
    "                    to the disease ({indications}) pathology. Describe the target's normal physiological function and any relevant\\\n",
    "                    biological pathways; Detail the target {targets}'s the tissue distribution (RNA or protein) and expression timing\\\n",
    "                    (e.g., fetal vs. adult, aging). Explain how the asset {asset_name} interacts with its direct biomolecular target\\\n",
    "                    (e.g., receptor binding); Specify the asset {asset_name}'s affinity, specificity, or selectivity for its target,\\\n",
    "                    and discuss how these properties may influence efficacy or safety.\n",
    "                    2. Pre-clinical Experimental Data: If company {company_name} has conducted any preclinical experiments for\\\n",
    "                    the asset {asset_name}, summarize the key preclinical experiments, covering pharmacodynamics, safety\\\n",
    "                    pharmacology, pharmacokinetics, and toxicology. Include detailed numerical data where possible, such as CMC\\\n",
    "                    attributes (physical, chemical, formulation, drug substance and product specifications), nonclinical\\\n",
    "                    study readouts (e.g., PD effect sizes, PK parameters like half-life and AUC, toxicology dose levels).\\\n",
    "                    If animal models were used to demonstrate the asset {asset_name}'s efficacy and safety, briefly describe\\\n",
    "                    the study design, current status or progress, and any remaining preclinical studies. If company\\\n",
    "                    {company_name} hasn't conducted any preclinical experiments for the asset {asset_name}, simply state that\\\n",
    "                    it hasn't conducted any preclinical experiments for asset {asset_name} yet.\"\"\"\n",
    "        \n",
    "        mechanism_questions.append({\n",
    "            \"question\": mechanism_question_text,\n",
    "            \"top_k_jd\": 100,\n",
    "            \"top_k_bm\": 100,\n",
    "            \"include_image\": False,\n",
    "            \"priority_order\": [\"jarvis_docs\"]\n",
    "        })\n",
    "        \n",
    "\n",
    "    # Generate clinical progress questions for each asset    \n",
    "    clinical_questions = []\n",
    "    for asset in company_info['assets']:\n",
    "        asset_name = asset['asset name']\n",
    "        stage = asset['current development stage']\n",
    "        clinical_question_text = f\"\"\"The asset {asset_name} in company {COMPANY_NAME}'s pipeline is in development stage {stage}.\\\n",
    "                            If the asset {asset_name} is in clinical stage, depending on the trial phase, provide the following:\\\n",
    "                            If the asset {asset_name} is in Phase 1, describe the study\\\n",
    "                            design (including randomization, blinding, and control arms if used), participant type and number\\\n",
    "                            (healthy volunteers or patients), primary endpoints, and summarize quantitative results\\\n",
    "                            for safety, dose-limiting toxicities, and tolerability; additionally, provide any\\\n",
    "                            exploratory changes of biomarker data or preliminary signals of efficacy with concrete numberic results. \\\n",
    "                            If the asset {asset_name} is in Phase 2, detail the study design (e.g., dose-finding, proof-of-concept), patient\\\n",
    "                            population and inclusion/exclusion criteria, primary and secondary endpoints (often initial\\\n",
    "                            efficacy and extended safety), and present efficacy and safety results using concrete quantitative\\\n",
    "                            metrics (response rates, changes in relevant biomarkers, mean differences, etc.), p-values, and\\\n",
    "                            interpret whether the results support advancement to pivotal trials.\\\n",
    "                            If the asset {asset_name} is in Phase 3, provide a comprehensive summary of the pivotal study design (randomization, control arms,\\\n",
    "                            multicenter participation), number of patients, detailed definitions of all primary and secondary\\\n",
    "                            endpoints, and full clinical results, including efficacy, safety, effect sizes, risk reductions,\\\n",
    "                            confidence intervals, and p-values, clearly assessing if the trial met its primary objectives and\\\n",
    "                            is likely to support regulatory approval or label expansion.\\\n",
    "                            For the trial, specify if the outcome was positive, negative, or inconclusive based on primary endpoints,\\\n",
    "                            and interpret the significance of the findings in light of current clinical standards\\\n",
    "                            and patient population needs. Note any missing data or information gaps.\\\n",
    "                            If the asset {asset_name} is not in clinical stage, simply state that it's not in clinical stage.\"\"\"\n",
    "        \n",
    "        clinical_questions.append({\n",
    "            \"question\": clinical_question_text,\n",
    "            \"top_k_jd\": 100,\n",
    "            \"top_k_bm\": 100,\n",
    "            \"include_image\": False,\n",
    "            \"priority_order\": [\"jarvis_docs\"]\n",
    "        })\n",
    "        \n",
    "        \n",
    "    # Generate market and commercial opportunity questions for each asset    \n",
    "    market_questions = []\n",
    "    for asset in company_info['assets']:\n",
    "        asset_name = asset['asset name']\n",
    "        indications = ', '.join(asset['targeted indications'])\n",
    "        market_question_text = f\"\"\"For company {company_name}'s asset {asset_name}'s targeted indication {indications}, estimate the diagnosed and\\\n",
    "                    total patient populations in the USA, Europe, and globally. Based on incidence and prevalence rates,\\\n",
    "                    note whether patient numbers are growing or declining, and define the therapy-eligible population\\\n",
    "                    considering line of therapy, disease stage, or biomarker subgroups. Estimate the global total\\\n",
    "                    addressable market (in U.S. dollars). Propose an annual price relative to the standard of care,\\\n",
    "                    estimate peak sales at a reasonable market-share penetration.\\\n",
    "                    Note that do not use the numbers provided by company {company_name}, use content from our Internal investment process docs.\"\"\"\n",
    "        \n",
    "        market_questions.append({\n",
    "            \"question\": market_question_text,\n",
    "            \"top_k_jd\": 100,\n",
    "            \"top_k_bm\": 100,\n",
    "            \"include_image\": False,\n",
    "            \"priority_order\": [\"jarvis_docs\"]\n",
    "        })\n",
    "\n",
    "        \n",
    "    # Generate asset-specific competitive landscape questions    \n",
    "    competitive_questions=[]\n",
    "    for asset in company_info['assets']:\n",
    "        asset_name = asset['asset name']\n",
    "        targets = ', '.join(asset['targets'])\n",
    "        indications = ', '.join(asset['targeted indications'])\n",
    "        therapeutic_areas = \", \".join(asset['targeted therapeutic areas'])\n",
    "        modality = asset['modality']\n",
    "        competitors = ', '.join(asset['companies with competing asset'])\n",
    "        competitor_valuation = '; '.join(asset['competitor_valuation'])\n",
    "        \n",
    "        competitive_landscape_text = f\"\"\"For asset {asset_name}, please provide a well‑organized response of its\\\n",
    "        Competitive Context: comprehensively list all biotech companies with active programs targeting {targets}\\\n",
    "        for indication {indications}, such as competitors {competitors}.\\\n",
    "        For each competitor, include: program, modality, clinical phase/status, key distinguishing features,\\\n",
    "        and financial information according to the following rules:\\\n",
    "        - If the competitor is a private company, provide its latest post-money valuation.\\\n",
    "        - If the competitor was acquired, provide the acquisition deal size.\\\n",
    "        - If the competitor is public, get its market cap via Google Finance.\\\n",
    "        Additional information: {competitor_valuation}.\\\n",
    "        Present the information in a structured table.\"\"\"\n",
    "        \n",
    "        strategic_interest_text = f\"\"\"For asset {asset_name}, please provide a well‑organized response of its Potential\\\n",
    "        Strategic Interest, where you identify large pharma companies, with pipelines in the same therapeutic areas\\\n",
    "        {therapeutic_areas}, that may be motivated to license or acquire the asset, especially those whose targets or modalities\\\n",
    "        differ from {targets} or {modality}. Structure your answer in a table and followed by a concise description.\"\"\"\n",
    "        \n",
    "        competitive_questions.append({\n",
    "            \"question\": competitive_landscape_text,\n",
    "            \"top_k_jd\": 100,\n",
    "            \"top_k_bm\": 100,\n",
    "            \"search_model\": \"o4-mini\",\n",
    "            \"include_image\": False,\n",
    "            \"priority_order\": [\"jarvis_docs\", \"online_search\"]\n",
    "        })\n",
    "        competitive_questions.append({\n",
    "            \"question\": strategic_interest_text,\n",
    "            \"top_k_jd\": 100,\n",
    "            \"top_k_bm\": 100,\n",
    "            \"search_model\": \"o4-mini\",\n",
    "            \"include_image\": False,\n",
    "            \"priority_order\": [\"jarvis_docs\", \"online_search\"]\n",
    "        }) \n",
    "        \n",
    "        \n",
    "    # Generate asset-specific market questions in appendix\n",
    "    append_market_questions=[]\n",
    "    for asset in company_info['assets']:\n",
    "        asset_name = asset['asset name']\n",
    "        targets = ', '.join(asset['targets'])\n",
    "        indications = ', '.join(asset['targeted indications'])\n",
    "        therapeutic_areas = \", \".join(asset['targeted therapeutic areas'])        \n",
    "        \n",
    "        China_market_text = f\"\"\"For asset {asset_name}'s targeted indication {indications}, estimate the diagnosed and\\\n",
    "        prevalent patient populations in China, and the population asset {asset_name} can target in China.\\        \n",
    "        Validate your data assumption with latest reports or literatures.\\\n",
    "        Show your estimation process step by step and list the reference links you used.\"\"\"\n",
    "        \n",
    "        USA_market_text = f\"\"\"For asset {asset_name}'s targeted indication {indications}, estimate the diagnosed and\\\n",
    "        prevalent patient populations in the USA, and the population asset {asset_name} can target in the USA.\\\n",
    "        Validate your data assumption with latest reports or literatures.\\\n",
    "        Show your estimation process step by step and list the reference links you used.\"\"\"\n",
    "\n",
    "        Global_market_text = f\"\"\"For asset {asset_name}'s targeted indication {indications}, estimate the diagnosed and\\\n",
    "        prevalent patient populations globally, and the population asset {asset_name} can target globally.\\\n",
    "        Validate your data assumption with latest reports or literatures.\\\n",
    "        Show your estimation process step by step and list the reference links you used.\"\"\"\n",
    "        \n",
    "        append_market_questions.append({\n",
    "            \"question\": China_market_text,\n",
    "            \"top_k_jd\": 5,\n",
    "            \"top_k_bm\": 5,\n",
    "            \"search_model\": \"o4-mini\",\n",
    "            \"include_image\": False,\n",
    "            \"priority_order\": [\"online_search\"]\n",
    "        })\n",
    "        append_market_questions.append({\n",
    "            \"question\": USA_market_text,\n",
    "            \"top_k_jd\": 5,\n",
    "            \"top_k_bm\": 5,\n",
    "            \"search_model\": \"o4-mini\",\n",
    "            \"include_image\": False,\n",
    "            \"priority_order\": [\"online_search\"]\n",
    "        })\n",
    "        append_market_questions.append({\n",
    "            \"question\": Global_market_text,\n",
    "            \"top_k_jd\": 5,\n",
    "            \"top_k_bm\": 5,\n",
    "            \"search_model\": \"o4-mini\",\n",
    "            \"include_image\": False,\n",
    "            \"priority_order\": [\"online_search\"]\n",
    "        }) \n",
    "        \n",
    "\n",
    "    # Generate asset-specific investment thesis questions    \n",
    "    invest_thesis_questions=[]\n",
    "    for asset in company_info['assets']:\n",
    "        asset_name = asset['asset name']\n",
    "        targets = ', '.join(asset['targets'])\n",
    "        indications = ', '.join(asset['targeted indications'])\n",
    "        modality = asset['modality']\n",
    "        stage = asset['current development stage']\n",
    "        brief_trial_result = asset['brief trial result']\n",
    "        \n",
    "        \n",
    "        invest_thesis_text = f\"\"\"Company {company_name}'s key asset {asset_name} is targeting indication {indications},\\\n",
    "        via {modality}, the asset {asset_name} is currently in development stage {stage}, and a brief trial result of asset\\\n",
    "        {asset_name} is as follows: {brief_trial_result}.\\\n",
    "        Research and provide 3 most compelling reasons as well as 3 key risks to invest in company {company_name}.\"\"\"\n",
    "                \n",
    "        # Create the question\n",
    "        invest_thesis_questions.append({\n",
    "            \"question\": invest_thesis_text,\n",
    "            \"top_k_jd\": 5,\n",
    "            \"top_k_bm\": 5,\n",
    "            \"search_model\": \"gpt-5\",\n",
    "            \"include_image\": False,\n",
    "            \"priority_order\": [\"online_search\"]\n",
    "        })\n",
    "        \n",
    "        \n",
    "    # Build the full JSON structure\n",
    "    json_questions_string = f\"\"\"\n",
    "{{\n",
    "    \"sections\": [\n",
    "        {{\n",
    "            \"section_title\": \"SECTION 1. Overview\",\n",
    "            \"content\": [\n",
    "                {{\n",
    "                    \"questions\": [\n",
    "                        {{\n",
    "                            \"question\": \"A brief description of the company {company_name}: Include information on when it was\\\n",
    "                            founded, its founders, (if it was spun out of any university or company, mention it as well),\\\n",
    "                            the core product (e.g., platform, top 2 leading drugs in pipeline), the\\\n",
    "                            stage of the key asset(s), the disease area(s) and indication(s) the company is targeting.\\\n",
    "                            Please provide a single, cohesive paragraph to address all the questions.\",\n",
    "                            \"top_k_jd\": 50, \"top_k_bm\": 50,\n",
    "                            \"include_image\": false,\n",
    "                            \"priority_order\": [\"jarvis_docs\"]\n",
    "                        }},\n",
    "                        {{\n",
    "                            \"question\": \"Write a brief summary recommendation explaining whether company {company_name}\\\n",
    "                            is an attractive investment opportunity for the current financing round.\\\n",
    "                            Please provide a single, cohesive paragraph to address all the questions.\",\n",
    "                            \"top_k_jd\": 100,\"top_k_bm\": 50,\n",
    "                            \"include_image\": false,\n",
    "                            \"priority_order\": [\"jarvis_docs\"]\n",
    "                        }}\n",
    "                    ]\n",
    "                }}\n",
    "            ]\n",
    "        }}, \n",
    "        {{\n",
    "            \"section_title\": \"SECTION 2. Deal dynamics\",\n",
    "            \"content\": [\n",
    "                {{\n",
    "                    \"questions\": [\n",
    "                        {{\n",
    "                            \"question\": \"Briefly state how we (Pivotal BioVenture Partners) learn about the company {company_name}.\\\n",
    "                            Then describe the previous financing history of company {company_name} prior to\\\n",
    "                            the current round. What rounds of funding has the company previously completed (size,\\\n",
    "                            investors etc.), and what were the key milestones achieved with each round? For company\\\n",
    "                            {company_name}'s most recent prior financing round, how much was raised and what were\\\n",
    "                            the pre-money and post-money valuation of the round?\\\n",
    "                            Please provide a single, cohesive paragraph to address all the questions.\",\n",
    "                            \"top_k_jd\": 100, \"top_k_bm\": 100,\n",
    "                            \"include_image\": false,\n",
    "                            \"priority_order\": [\"jarvis_docs\"]\n",
    "                        }},\n",
    "                        {{\n",
    "                            \"question\": \"Outline the deal terms associated with company {company_name}'s current\\\n",
    "                            financing round.\",\n",
    "                            \"top_k_jd\": 50,\"top_k_bm\": 50,\n",
    "                            \"include_image\": false,\n",
    "                            \"priority_order\": [\"jarvis_docs\"]\n",
    "                        }}                       \n",
    "                    ]\n",
    "                }}\n",
    "            ]\n",
    "        }}, \n",
    "        {{\n",
    "            \"section_title\": \"SECTION 3. Platform\",\n",
    "            \"content\": [\n",
    "                {{\n",
    "                    \"questions\": [\n",
    "                        {{\n",
    "                            \"question\": \"If company {company_name} is a Platform-Based company or has a technology\\\n",
    "                            platform underpinning its assets or discovery efforts, describe the platform. If the\\\n",
    "                            platform solves or addresses a problem primarily in any of the following areas, please\\\n",
    "                            concretely describe (avoid generic description): drug discovery, target discovery,\\\n",
    "                            target biology, drug-target biology and/or binding, preclinical experiment prediction,\\\n",
    "                            preclinical experiment efficiencies, clinical trial prediction, clinical trial\\\n",
    "                            efficiencies, patient selection. If the platform does not solve or address a problem in\\\n",
    "                            any of the aforementioned areas, what unique or novel insight is unlocked by the\\\n",
    "                            platform? If the company is not a Platform-Based company, skip this question and simply\\\n",
    "                            return the answer as 'Company {company_name} is not a Platform-Based company or does\\\n",
    "                            not have a platform'.\",\n",
    "                            \"top_k_jd\": 100, \"top_k_bm\": 50,\n",
    "                            \"include_image\": false,\n",
    "                            \"priority_order\": [\"jarvis_docs\"]\n",
    "                        }}\"\"\" + (f\"\"\",\n",
    "                        {{\n",
    "                            \"question\": \"Give a table of company {company_name} and its main competitor companies.\\\n",
    "                            Include information such as the name of the company, the year the company was founded,\\\n",
    "                            the company's core technology, the furthest drug discovery or development\\\n",
    "                            stage the company has reached, the key scientist(s) and their affiliation of the company,\\\n",
    "                            pipeline highlights of the company, key investors, total capital raised to date of the company,\\\n",
    "                            most recent round's post-money valuation (or current market cap for public company).\",\n",
    "                            \"top_k_jd\": 50,\"top_k_bm\": 50,\n",
    "                            \"search_model\": \"o4-mini\",\n",
    "                            \"include_image\": false,\n",
    "                            \"priority_order\": [\"jarvis_docs\",\"online_search\"]\n",
    "                        }}\"\"\" if company_info.get('platform is core asset', False) else \"\") + f\"\"\"                       \n",
    "                    ]\n",
    "                }}\n",
    "            ]\n",
    "        }},                             \n",
    "        {{\n",
    "            \"section_title\": \"SECTION 4. Product summary and pipeline analysis\",\n",
    "            \"content\": [\n",
    "                {{\n",
    "                    \"title\": \"Pipeline overview\",\n",
    "                    \"questions\": [\n",
    "                        {{\n",
    "                            \"question\": \"Concisely describe the drug pipeline of company {company_name}: include the name(s) of\\\n",
    "                            the program(s), the specific molecular target, the modality (e.g., small molecule, antibody, etc.),\\\n",
    "                            route of administration, the proposed indication(s) and current development status (e.g.,\\\n",
    "                            preclinical, phase 1, phase 2, etc.).\\\n",
    "                            Please provide a single, cohesive paragraph to address all the questions.\",\n",
    "                            \"top_k_jd\": 50,\"top_k_bm\": 50,\n",
    "                            \"include_image\": false,\n",
    "                            \"priority_order\": [\"jarvis_docs\"]\n",
    "                        }},                    \n",
    "                        {{\n",
    "                            \"question\": \"If the key asset(s) in company {company_name} was/were in-licensed: Provide a brief statement\\\n",
    "                            on where it was licensed from, the previous naming of the drug, when it was licensed, and describe\\\n",
    "                            the results/outcomes from previous preclinical and/or clinical studies (including max phase/trial,\\\n",
    "                            number of patients in the max trial, efficacy results with p-values, and safety results/concerns)\\\n",
    "                            by the originator.\\\n",
    "                            If the key asset(s) was/were not licensed, skip this question and simply return the key asset(s)\\\n",
    "                            in company {company_name} was/were not in-licensed as the answer.\\\n",
    "                            Please provide a single, cohesive paragraph to address all the questions.\",\n",
    "                            \"top_k_jd\": 50, \"top_k_bm\": 50,\n",
    "                            \"include_image\": false,\n",
    "                            \"priority_order\": [\"jarvis_docs\"]\n",
    "                        }}\n",
    "                    ]\n",
    "                }},  \n",
    "                {{\n",
    "                    \"title\": \"Mechanism and pre-clinical result\",\n",
    "                    \"questions\": {json.dumps(mechanism_questions)}\n",
    "                }},\n",
    "                {{\n",
    "                    \"title\": \"Clinical progress\",\n",
    "                    \"questions\": {json.dumps(clinical_questions)}                 \n",
    "                }},\n",
    "                {{\n",
    "                    \"title\": \"Market and commercial opportunity\",\n",
    "                    \"questions\": {json.dumps(market_questions)}\n",
    "                }},                        \n",
    "                {{\n",
    "                    \"title\": \"Competitive landscape and potential strategic interest\",\n",
    "                    \"questions\": {json.dumps(competitive_questions)}\n",
    "                }}                                                                   \n",
    "            ]\n",
    "        }}, \n",
    "        {{\n",
    "            \"section_title\": \"SECTION 5. Company key milestones\",\n",
    "            \"content\": [\n",
    "                {{\n",
    "                    \"questions\": [\n",
    "                        {{\n",
    "                            \"question\": \"Describe {company_name}'s planned milestones, with a timeline detailed by year and\\\n",
    "                            quarter. Then provide the cash runway timing that will be achieved with the current financing.\\\n",
    "                            Please provide a single, cohesive paragraph to address all the questions.\",\n",
    "                            \"top_k_jd\": 100, \"top_k_bm\": 100,\n",
    "                            \"include_image\": false,\n",
    "                            \"priority_order\": [\"jarvis_docs\"]\n",
    "                        }}\n",
    "                    ]\n",
    "                }}\n",
    "            ]\n",
    "        }},                                     \n",
    "        {{\n",
    "            \"section_title\": \"SECTION 6. Recommendations and Next Steps\",\n",
    "            \"content\": [\n",
    "                {{\n",
    "                    \"questions\": [\n",
    "                        {{\n",
    "                            \"question\": \"What specific due diligence steps would you recommend next for evaluating the\\\n",
    "                            opportunity to invest in {company_name}? Please give concrete, well-reasoned suggestions based on\\\n",
    "                            the provided content, and avoid any vague or general descriptions.\\\n",
    "                            Provide your answer as a single, cohesive paragraph.\",\n",
    "                            \"top_k_jd\": 100, \"top_k_bm\": 50,\n",
    "                            \"include_image\": false,\n",
    "                            \"priority_order\": [\"jarvis_docs\",\"online_search\"]\n",
    "                        }}\n",
    "                    ]\n",
    "                }}\n",
    "            ]\n",
    "        }},               \n",
    "        {{\n",
    "            \"section_title\": \"Appendix\",\n",
    "            \"content\": [\n",
    "                {{\n",
    "                    \"title\": \"Investment thesis and risks by AI\",\n",
    "                    \"questions\": {json.dumps(invest_thesis_questions)}\n",
    "                }},                                            \n",
    "                {{\n",
    "                    \"title\": \"Management team\",\n",
    "                    \"questions\": [\n",
    "                        {{\n",
    "                            \"question\": \"Provide a section for each senior team member of company {company_name} (Include any C-level executives\\\n",
    "                            or founders, members of the board of directors, and members of the clinical/scientific advisory board,\\\n",
    "                            and, if needed, any Senior Vice President level executives.) answering the following\\\n",
    "                            questions. Provide the answers together a whole paragraph as a text piece, not bullet form.\\\n",
    "                            Education and Credentials: What are the educational backgrounds and professional credentials\\\n",
    "                            of the senior team member?\\\n",
    "                            Experience: What is the professional background and experience of each senior team member?\\\n",
    "                            Track Record: What notable achievements or successes have the senior team members had in\\\n",
    "                            their previous roles?\\\n",
    "                            Relevant Expertise: How does the expertise of each team member align with the company's\\\n",
    "                            strategic goals and needs?\\\n",
    "                            Leadership Skills: What are the demonstrated leadership qualities of each senior team\\\n",
    "                            member?\\\n",
    "                            If any of the senior team members mentioned above are new or incoming hires,\\\n",
    "                            please list them separately.\",\n",
    "                            \"top_k_jd\": 50, \"top_k_bm\": 50,\n",
    "                            \"include_image\": false,\n",
    "                            \"priority_order\": [\"jarvis_docs\"]\n",
    "                        }}                  \n",
    "                    ]\n",
    "                }}\n",
    "            ]\n",
    "        }}\n",
    "    ]\n",
    "}}\n",
    "\"\"\"\n",
    "    \n",
    "    return json_questions_string\n",
    "\n",
    "\n",
    "#                 {{\n",
    "#                     \"title\": \"Market analysis by AI\",\n",
    "#                     \"questions\": {json.dumps(append_market_questions)}\n",
    "#                 }},                \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59fd4664",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test\n",
    "COMPANY_NAME = \"PPInnova (Peak Perform Innova)\"\n",
    "json_questions_string = generate_questions_with_assets(COMPANY_NAME, company_info)\n",
    "\n",
    "# Parse to verify it's valid JSON\n",
    "questions_dict = json.loads(json_questions_string)\n",
    "\n",
    "# Print just the competitive landscape section to verify\n",
    "for section in questions_dict['sections']:\n",
    "    if section['section_title'] == 'SECTION 5. Product summary and pipeline analysis':\n",
    "        for content in section['content']:\n",
    "            if isinstance(content, dict) and content.get('title') == 'Competitive landscape and potential strategic interest':\n",
    "                print(content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11e6d446",
   "metadata": {},
   "outputs": [],
   "source": [
    "COMPANY_NAME = \"PPInnova (Peak Perform Innova)\"\n",
    "json_questions_string = generate_questions_with_assets(COMPANY_NAME, company_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db08c4c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "now = datetime.now(pytz.timezone('Asia/Shanghai')).date()\n",
    "with open(f\"json_questions_string_factsheet_{now}.txt\", 'w') as file:\n",
    "    file.write(json_questions_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b67e27bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open(\"json_questions_string_2025-05-15.txt\", 'r') as file:\n",
    "#     json_questions_string = file.read()\n",
    "# json_questions = json.loads(json_questions_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7a6c298",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "json_questions = json.loads(json_questions_string)\n",
    "json_questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a5c4776",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1ddf0082",
   "metadata": {},
   "source": [
    "## regenerate outputs for investors easily to see prompts for each sections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0193913",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gather_responses(content, db_jd, bm25_retriever):\n",
    "    \"\"\"\n",
    "    Process content items and gather responses for questions and subsections.\n",
    "    \n",
    "    Args:\n",
    "        content (list): List of content items containing questions and subsections\n",
    "        db_jd (object): Database for Jarvis documents (vector store)\n",
    "        bm25_retriever (object): BM25 retriever for keyword-based search\n",
    "        \n",
    "    Returns:\n",
    "        list: Structured responses for the content\n",
    "    \"\"\"\n",
    "    section_responses = []\n",
    "    \n",
    "    for item in content:\n",
    "        subtitle = item.get('title')\n",
    "        questions = item.get('questions', [])\n",
    "        subsections = item.get('subsections', [])\n",
    "        \n",
    "        section_combined_response_with_prompt = \"\"\n",
    "        section_combined_response_without_prompt = \"\"\n",
    "        section_combined_source_counts = {}\n",
    "        section_combined_images = []\n",
    "        section_online_search_responses = []  # Store online search responses\n",
    "        subsection_responses = []\n",
    "\n",
    "        if questions:\n",
    "            for question in questions:\n",
    "                detailed_question = question['question']\n",
    "                k_jd = question['top_k_jd']\n",
    "                k_bm = question['top_k_bm']\n",
    "                search_model = question.get('search_model', 'o4-mini')\n",
    "                include_image = question['include_image']\n",
    "                priority_order = question['priority_order']\n",
    "                \n",
    "                # Process the response using the ensemble function\n",
    "                response, online_search_response = answer_with_search_ensemble(\n",
    "                    detailed_question, \n",
    "                    bm25_retriever, \n",
    "                    k_bm, \n",
    "                    db_jd, \n",
    "                    k_jd, \n",
    "                    search_model,\n",
    "                    priority_order\n",
    "                )\n",
    "                \n",
    "                # Store the online search response\n",
    "                section_online_search_responses.append({\n",
    "                    \"question\": detailed_question, \n",
    "                    \"online_search_response\": online_search_response\n",
    "                })\n",
    "                \n",
    "                # Include the question in the response output with prompt\n",
    "                question_text = f\"**Q: {detailed_question}**\\n\"\n",
    "                section_combined_response_with_prompt += question_text + response + \"\\n\\n\"\n",
    "                \n",
    "                # Include the response only in the output without prompt\n",
    "                section_combined_response_without_prompt += response + \"\\n\\n\"\n",
    "                \n",
    "                # Track which sources were used (simplified version)\n",
    "                for source in priority_order:\n",
    "                    if source == 'jarvis_docs':\n",
    "                        section_combined_source_counts[source] = section_combined_source_counts.get(source, 0) + 1\n",
    "                    elif source == 'online_search' and online_search_response:\n",
    "                        section_combined_source_counts['external_sources'] = section_combined_source_counts.get('external_sources', 0) + 1\n",
    "\n",
    "                # Handle images if needed (placeholder - adjust based on your actual image handling)\n",
    "                if include_image:\n",
    "                    # section_combined_images.extend(overview_images)\n",
    "                    pass\n",
    "\n",
    "        if subsections:\n",
    "            for subsection in subsections:\n",
    "                subsubtitle = subsection['title']\n",
    "                subquestions = subsection['questions']\n",
    "\n",
    "                sub_combined_response_with_prompt = \"\"\n",
    "                sub_combined_response_without_prompt = \"\"\n",
    "                sub_combined_source_counts = {}\n",
    "                sub_combined_images = []\n",
    "                sub_online_search_responses = []\n",
    "\n",
    "                for subquestion in subquestions:\n",
    "                    detailed_question = subquestion['question']\n",
    "                    k_jd = subquestion['top_k_jd']\n",
    "                    k_bm = subquestion['top_k_bm']\n",
    "                    search_model = subquestion.get('search_model', 'o4-mini')\n",
    "                    include_image = subquestion['include_image']\n",
    "                    priority_order = subquestion['priority_order']\n",
    "                    \n",
    "                    # Process the response\n",
    "                    response, online_search_response = answer_with_search_ensemble(\n",
    "                        detailed_question, \n",
    "                        bm25_retriever, \n",
    "                        k_bm, \n",
    "                        db_jd, \n",
    "                        k_jd, \n",
    "                        search_model,\n",
    "                        priority_order\n",
    "                    )\n",
    "                    \n",
    "                    # Store the online search response\n",
    "                    sub_online_search_responses.append({\n",
    "                        \"question\": detailed_question, \n",
    "                        \"online_search_response\": online_search_response\n",
    "                    })\n",
    "                    \n",
    "                    # Include the question in the response output with prompt\n",
    "                    question_text = f\"**Q: {detailed_question}**\\n\"\n",
    "                    sub_combined_response_with_prompt += question_text + response + \"\\n\\n\"\n",
    "                    \n",
    "                    # Include the response only in the output without prompt\n",
    "                    sub_combined_response_without_prompt += response + \"\\n\\n\"\n",
    "                    \n",
    "                    # Track sources\n",
    "                    for source in priority_order:\n",
    "                        if source == 'jarvis_docs':\n",
    "                            sub_combined_source_counts[source] = sub_combined_source_counts.get(source, 0) + 1\n",
    "                        elif source == 'online_search' and online_search_response:\n",
    "                            sub_combined_source_counts['external_sources'] = sub_combined_source_counts.get('external_sources', 0) + 1\n",
    "\n",
    "                    # Handle images if needed\n",
    "                    if include_image:\n",
    "                        # sub_combined_images.extend(overview_images)\n",
    "                        pass\n",
    "\n",
    "                subsection_responses.append((\n",
    "                    subsubtitle,\n",
    "                    sub_combined_response_with_prompt,\n",
    "                    sub_combined_response_without_prompt,\n",
    "                    sub_combined_source_counts,\n",
    "                    sub_combined_images,\n",
    "                    any(q['include_image'] for q in subquestions),\n",
    "                    sub_online_search_responses\n",
    "                ))\n",
    "\n",
    "        section_responses.append((\n",
    "            subtitle, \n",
    "            section_combined_response_with_prompt, \n",
    "            section_combined_response_without_prompt, \n",
    "            subsection_responses, \n",
    "            section_combined_source_counts, \n",
    "            section_combined_images, \n",
    "            any(q['include_image'] for q in questions) if questions else False,\n",
    "            section_online_search_responses\n",
    "        ))\n",
    "\n",
    "    return section_responses\n",
    "\n",
    "\n",
    "def gather_all_responses(json_questions_string, db_jd, bm25_retriever):\n",
    "    \"\"\"\n",
    "    Process a JSON string containing questions and generate responses.\n",
    "    \n",
    "    Args:\n",
    "        json_questions_string (str): JSON string containing sections, questions, and parameters\n",
    "        db_jd (object): Database for Jarvis documents (vector store)\n",
    "        bm25_retriever (object): BM25 retriever for keyword-based search\n",
    "        \n",
    "    Returns:\n",
    "        list: Structured responses for all sections with their questions and answers\n",
    "    \"\"\"\n",
    "    json_questions = json.loads(json_questions_string)\n",
    "    all_sections_responses = []\n",
    "\n",
    "    # Process each section\n",
    "    for section in json_questions[\"sections\"]:\n",
    "        section_title = section[\"section_title\"]\n",
    "        section_content = section[\"content\"]\n",
    "        \n",
    "        # Gather responses for this section\n",
    "        section_responses = gather_responses(section_content, db_jd, bm25_retriever)\n",
    "        \n",
    "        # Store the section title and its responses\n",
    "        all_sections_responses.append({\n",
    "            \"section_title\": section_title,\n",
    "            \"content\": section_responses\n",
    "        })\n",
    "    \n",
    "    return all_sections_responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6410b3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_image(image_data, image_name):\n",
    "    sanitized_image_name = re.sub(r'[^a-zA-Z0-9_\\-\\.]', '_', image_name)\n",
    "    image_path = os.path.join(image_dir, sanitized_image_name)\n",
    "    os.makedirs(os.path.dirname(image_path), exist_ok=True)\n",
    "    with open(image_path, \"wb\") as img_file:\n",
    "        img_file.write(base64.b64decode(image_data))\n",
    "    return image_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fb485fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_sections_responses = gather_all_responses(json_questions_string, db_jd, bm25_retriever)\n",
    "\n",
    "now = datetime.now(pytz.timezone('Asia/Shanghai')).date()\n",
    "#responses_file_path = f\"all_sections_responses_o4mini_{now}.pkl\"\n",
    "responses_file_path = f\"all_sections_responses_gpt41_factsheet_{now}.pkl\"\n",
    "#responses_file_path = f\"all_sections_responses_o3_{now}.pkl\"\n",
    "\n",
    "# Save the responses_with_sources object to a file\n",
    "with open(responses_file_path, \"wb\") as file:\n",
    "    pickle.dump(all_sections_responses, file)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9af5d761",
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open('all_sections_responses_gpt41_factsheet_2025-08-28.pkl', \"rb\") as file:\n",
    "#     all_sections_responses = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef9947d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_any_section_or_subsection(json_questions_string, existing_responses_path,\n",
    "                                    section_title, subsection_title, \n",
    "                                    db_jd, bm25_retriever):\n",
    "    \"\"\"\n",
    "    Universal function to update either a section or a subsection within a section.\n",
    "    \n",
    "    Args:\n",
    "        section_title: The main section title (e.g., \"SECTION 1. Overview\" or \"Appendix\")\n",
    "        subsection_title: The subsection title if applicable (e.g., \"Market analysis by AI\"),\n",
    "                         or None if updating a section without subsections\n",
    "    \"\"\"\n",
    "    # Load existing responses\n",
    "    with open(existing_responses_path, \"rb\") as file:\n",
    "        all_sections_responses = pickle.load(file)\n",
    "    \n",
    "    # Parse the questions JSON\n",
    "    json_questions = json.loads(json_questions_string)\n",
    "    \n",
    "    # Find the section in questions\n",
    "    target_section = None\n",
    "    for section in json_questions[\"sections\"]:\n",
    "        if section[\"section_title\"] == section_title:\n",
    "            target_section = section\n",
    "            break\n",
    "    \n",
    "    if target_section is None:\n",
    "        print(f\"Could not find section: {section_title}\")\n",
    "        return all_sections_responses\n",
    "        \n",
    "    if subsection_title is None:\n",
    "        # Update entire section (no subsection specified)\n",
    "        new_responses = gather_responses(target_section[\"content\"], db_jd, bm25_retriever)\n",
    "        \n",
    "        # Replace the entire content for this section\n",
    "        for section in all_sections_responses:\n",
    "            if section[\"section_title\"] == section_title:\n",
    "                section[\"content\"] = new_responses\n",
    "                print(f\"Updated entire section: {section_title}\")\n",
    "                break\n",
    "    else:\n",
    "        # Update specific subsection within a section\n",
    "        # Find the specific subsection questions\n",
    "        target_questions = None\n",
    "        for item in target_section[\"content\"]:\n",
    "            if item.get(\"title\") == subsection_title:\n",
    "                target_questions = item.get(\"questions\", [])\n",
    "                break\n",
    "        \n",
    "        if target_questions is None:\n",
    "            print(f\"Could not find subsection '{subsection_title}' in section '{section_title}'\")\n",
    "            return all_sections_responses\n",
    "        \n",
    "        # Create temporary content for gathering\n",
    "        temp_content = [{\n",
    "            \"title\": subsection_title,\n",
    "            \"questions\": target_questions\n",
    "        }]\n",
    "        \n",
    "        # Get new responses for this subsection\n",
    "        new_responses = gather_responses(temp_content, db_jd, bm25_retriever)\n",
    "        \n",
    "        # Update the specific subsection\n",
    "        for section in all_sections_responses:\n",
    "            if section[\"section_title\"] == section_title:\n",
    "                for i, content_item in enumerate(section[\"content\"]):\n",
    "                    if content_item[0] == subsection_title:\n",
    "                        section[\"content\"][i] = new_responses[0]\n",
    "                        print(f\"Updated subsection '{subsection_title}' in section '{section_title}'\")\n",
    "                        break\n",
    "                break\n",
    "    \n",
    "    return all_sections_responses\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e6f003b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_file(file_path, include_sources, include_prompts, all_sections_responses):\n",
    "    with open(file_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        \n",
    "        f.write(\"<style>\\n\")\n",
    "        font_settings = {\"size\": \"11px\",\n",
    "                         \"family\": \"Arial\",  # Alternative: \"Calibri\"\n",
    "                         \"weight\": \"500\"\n",
    "                        }\n",
    "        f.write(f\"body {{ font-size: {font_settings['size']}; font-family: {font_settings['family']}; font-weight: {font_settings['weight']};}}\\n\")\n",
    "        f.write(\"</style>\\n\\n\")\n",
    "        \n",
    "        f.write(f\"# Fact Sheet -- **{COMPANY_NAME}**\\n\\n\")\n",
    "        \n",
    "        # Generate Table of Contents\n",
    "        f.write(\"# Table of Contents\\n\\n\")\n",
    "        for i, section in enumerate(all_sections_responses):\n",
    "            section_title = section['section_title']\n",
    "            section_anchor = f\"section-{i+1}\"\n",
    "            f.write(f\"- [{section_title}](#{section_anchor})\\n\")\n",
    "            section_content = section['content']\n",
    "            \n",
    "            # Add subsections to the TOC only if valid titles exist\n",
    "            for j, (subtitle, _, _, subsection_responses, _, _, _, _) in enumerate(section_content):\n",
    "                if subtitle:  # Add valid subtitles\n",
    "                    subsection_anchor = f\"{section_anchor}-subsection-{j+1}\"\n",
    "                    f.write(f\"  - [{subtitle}](#{subsection_anchor})\\n\")\n",
    "                    for k, (subsubtitle, _, _, _, _, _) in enumerate(subsection_responses):\n",
    "                        subsubsection_anchor = f\"{subsection_anchor}-subsubsection-{k+1}\"\n",
    "                        f.write(f\"    - [{subsubtitle}](#{subsubsection_anchor})\\n\")\n",
    "        f.write(\"\\n\")\n",
    "\n",
    "        # Write each section and subsection\n",
    "        for i, section in enumerate(all_sections_responses):\n",
    "            section_title = section['section_title']\n",
    "            section_content = section['content']\n",
    "            section_anchor = f\"section-{i+1}\"\n",
    "\n",
    "            # Section title\n",
    "            f.write(f\"## <a id='{section_anchor}'></a>**{section_title}**\\n\\n\")\n",
    "\n",
    "            for j, (subtitle, section_response_with_prompt, section_response_without_prompt, subsection_responses, section_combined_source_counts, section_images, include_image, online_search_responses) in enumerate(section_content):\n",
    "                subsection_anchor = f\"{section_anchor}-subsection-{j+1}\"\n",
    "                \n",
    "                # Add subtitle if it exists\n",
    "                if subtitle:\n",
    "                    f.write(f\"### <a id='{subsection_anchor}'></a>**{subtitle}**\\n\\n\")\n",
    "                \n",
    "                # Select whether to include prompts or not\n",
    "                if include_prompts:\n",
    "                    f.write(f\"{section_response_with_prompt}\\n\")\n",
    "                else:\n",
    "                    f.write(f\"{section_response_without_prompt}\\n\")\n",
    "                \n",
    "#                 # Add sources if requested\n",
    "#                 if include_sources and section_combined_source_counts:\n",
    "#                     f.write(\"\\n\\n#### _Document Source and Counts_\\n\")\n",
    "#                     for source, count in section_combined_source_counts.items():\n",
    "#                         f.write(f\"<p style='margin: 2px; font-size:small;'><i>- {source}: {count} occurrence(s)</i></p>\\n\")\n",
    "\n",
    "                # Add sources **right after the specific response**, not at section level\n",
    "                if include_sources and section_combined_source_counts:\n",
    "                    f.write(\"\\n\\n#### _Document Source and Counts_\\n\")\n",
    "                    for source, count in section_combined_source_counts.items():\n",
    "                        f.write(f\"<p style='margin: 2px; font-size:small;'><i>- {source}: {count} occurrence(s)</i></p>\\n\")\n",
    "\n",
    "                f.write(\"\\n\")  # Ensure spacing before the next response\n",
    "\n",
    "                # Add images if requested\n",
    "                if include_image and section_images:\n",
    "                    f.write(\"\\n\\n#### _Related Images_\\n\")\n",
    "                    for idx, image_data in enumerate(section_images):\n",
    "                        image_name = f\"{subtitle.replace(' ', '_')}_{idx}.png\"\n",
    "                        image_path = save_image(image_data, image_name)\n",
    "                        relative_image_path = os.path.relpath(image_path, start=os.path.dirname(file_path))\n",
    "                        f.write(f\"![Image]({relative_image_path})\\n\")\n",
    "                f.write(\"\\n\")\n",
    "\n",
    "                # Process subsection responses\n",
    "                for k, (subsubtitle, subresponse_with_prompt, subresponse_without_prompt, subsource_counts, subimages, subinclude_image) in enumerate(subsection_responses):\n",
    "                    subsubsection_anchor = f\"{subsection_anchor}-subsubsection-{k+1}\"\n",
    "                    f.write(f\"#### <a id='{subsubsection_anchor}'></a>**{subsubtitle}**\\n\\n\")\n",
    "                    \n",
    "                    # Select whether to include prompts or not\n",
    "                    if include_prompts:\n",
    "                        f.write(f\"{subresponse_with_prompt}\\n\\n\")\n",
    "                    else:\n",
    "                        f.write(f\"{subresponse_without_prompt}\\n\\n\")\n",
    "                    \n",
    "                    # Add sources if requested\n",
    "                    if include_sources and subsource_counts:\n",
    "                        f.write(\"\\n\\n#### _Document Source and Counts_\\n\")\n",
    "                        for source, count in subsource_counts.items():\n",
    "                            f.write(f\"<p style='margin: 2px; font-size:small;'><i>- {source}: {count} occurrence(s)</i></p>\\n\")\n",
    "                    \n",
    "                    # Add images if requested\n",
    "                    if subinclude_image and subimages:\n",
    "                        f.write(\"\\n\\n#### _Related Images_\\n\")\n",
    "                        for idx, image_data in enumerate(subimages):\n",
    "                            image_name = f\"{subsubtitle.replace(' ', '_')}_{idx}.png\"\n",
    "                            image_path = save_image(image_data, image_name)\n",
    "                            relative_image_path = os.path.relpath(image_path, start=os.path.dirname(file_path))\n",
    "                            f.write(f\"![Image]({relative_image_path})\\n\")\n",
    "                    f.write(\"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e69fff9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "COMPANY_NAME = \"PPInnova\"\n",
    "now = datetime.now(pytz.timezone('Asia/Shanghai')).date()\n",
    "#now = '2025-08-25'\n",
    "\n",
    "OUTPUT_PATH_WITH_PROMPTS_WITH_SOURCE = f\"{COMPANY_NAME}_factsheet_with_prompts_with_source_gpt41_{now}.md\"\n",
    "OUTPUT_PATH_WITH_PROMPTS_WITHOUT_SOURCE = f\"{COMPANY_NAME}_factsheet_with_prompts_without_source_gpt41_{now}.md\"\n",
    "OUTPUT_PATH_WITHOUT_PROMPTS_WITH_SOURCE = f\"{COMPANY_NAME}_factsheet_with_source_gpt41_{now}.md\"\n",
    "OUTPUT_PATH_WITHOUT_PROMPTS_WITHOUT_SOURCE = f\"{COMPANY_NAME}_factsheet_without_source_gpt41_{now}.md\"\n",
    "\n",
    "image_dir = f\"{os.getcwd()}/images_for_factsheet_generation\"    \n",
    "\n",
    "write_file(OUTPUT_PATH_WITH_PROMPTS_WITH_SOURCE, include_sources=True, include_prompts=True, all_sections_responses=all_sections_responses)\n",
    "write_file(OUTPUT_PATH_WITH_PROMPTS_WITHOUT_SOURCE, include_sources=False, include_prompts=True, all_sections_responses=all_sections_responses)\n",
    "write_file(OUTPUT_PATH_WITHOUT_PROMPTS_WITH_SOURCE, include_sources=True, include_prompts=False, all_sections_responses=all_sections_responses)\n",
    "write_file(OUTPUT_PATH_WITHOUT_PROMPTS_WITHOUT_SOURCE, include_sources=False, include_prompts=False, all_sections_responses=all_sections_responses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c0ca075",
   "metadata": {},
   "outputs": [],
   "source": [
    "# COMPANY_NAME = \"RiboX\"\n",
    "# now = datetime.now(pytz.timezone('Asia/Shanghai')).date()\n",
    "# #now = '2025-05-15'\n",
    "# OUTPUT_PATH_WITH_PROMPTS_WITH_SOURCE = f\"{COMPANY_NAME}_factsheet_with_prompts_with_source_o4mini_{now}.md\"\n",
    "# OUTPUT_PATH_WITH_PROMPTS_WITHOUT_SOURCE = f\"{COMPANY_NAME}_factsheet_with_prompts_without_source_o4mini_{now}.md\"\n",
    "# OUTPUT_PATH_WITHOUT_PROMPTS_WITH_SOURCE = f\"{COMPANY_NAME}_factsheet_with_source_o4mini_{now}.md\"\n",
    "# OUTPUT_PATH_WITHOUT_PROMPTS_WITHOUT_SOURCE = f\"{COMPANY_NAME}_factsheet_without_source_o4mini_{now}.md\"\n",
    "\n",
    "# image_dir = f\"{os.getcwd()}/images_for_factsheet_generation\"    \n",
    "\n",
    "# write_file(OUTPUT_PATH_WITH_PROMPTS_WITH_SOURCE, include_sources=True, include_prompts=True, all_sections_responses=all_sections_responses)\n",
    "# write_file(OUTPUT_PATH_WITH_PROMPTS_WITHOUT_SOURCE, include_sources=False, include_prompts=True, all_sections_responses=all_sections_responses)\n",
    "# write_file(OUTPUT_PATH_WITHOUT_PROMPTS_WITH_SOURCE, include_sources=True, include_prompts=False, all_sections_responses=all_sections_responses)\n",
    "# write_file(OUTPUT_PATH_WITHOUT_PROMPTS_WITHOUT_SOURCE, include_sources=False, include_prompts=False, all_sections_responses=all_sections_responses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "521fa349",
   "metadata": {},
   "outputs": [],
   "source": [
    "# COMPANY_NAME = \"RiboX\"\n",
    "# now = datetime.now(pytz.timezone('Asia/Shanghai')).date()\n",
    "# #now = '2025-05-15'\n",
    "\n",
    "# OUTPUT_PATH_WITH_PROMPTS_WITH_SOURCE = f\"{COMPANY_NAME}_factsheet_with_prompts_with_source_o3_{now}.md\"\n",
    "# OUTPUT_PATH_WITH_PROMPTS_WITHOUT_SOURCE = f\"{COMPANY_NAME}_factsheet_with_prompts_without_source_o3_{now}.md\"\n",
    "# OUTPUT_PATH_WITHOUT_PROMPTS_WITH_SOURCE = f\"{COMPANY_NAME}_factsheet_with_source_o3_{now}.md\"\n",
    "# OUTPUT_PATH_WITHOUT_PROMPTS_WITHOUT_SOURCE = f\"{COMPANY_NAME}_factsheet_without_source_o3_{now}.md\"\n",
    "\n",
    "# image_dir = f\"{os.getcwd()}/images_for_factsheet_generation\"    \n",
    "\n",
    "# write_file(OUTPUT_PATH_WITH_PROMPTS_WITH_SOURCE, include_sources=True, include_prompts=True, all_sections_responses=all_sections_responses)\n",
    "# write_file(OUTPUT_PATH_WITH_PROMPTS_WITHOUT_SOURCE, include_sources=False, include_prompts=True, all_sections_responses=all_sections_responses)\n",
    "# write_file(OUTPUT_PATH_WITHOUT_PROMPTS_WITH_SOURCE, include_sources=True, include_prompts=False, all_sections_responses=all_sections_responses)\n",
    "# write_file(OUTPUT_PATH_WITHOUT_PROMPTS_WITHOUT_SOURCE, include_sources=False, include_prompts=False, all_sections_responses=all_sections_responses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1425ceaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_prompts(file_path, json_questions_string):\n",
    "    \"\"\"\n",
    "    Generates a Markdown file listing all the questions by section from the JSON string.\n",
    "    \n",
    "    Parameters:\n",
    "      - file_path (str): The output file path for the Markdown file.\n",
    "      - json_questions_string (str): A JSON string containing the sections and questions.\n",
    "    \"\"\"\n",
    "    import json  # Ensure json is imported.\n",
    "    data = json.loads(json_questions_string)\n",
    "    \n",
    "    def write_questions(f, questions):\n",
    "        \"\"\"Helper function to write questions from a list.\"\"\"\n",
    "        for question in questions:\n",
    "            # Get the question text and strip any extra whitespace.\n",
    "            prompt = question.get('question', '').strip()\n",
    "            f.write(f\"**Question**: {prompt}\\n\\n\")\n",
    "            # Write out the priority order for the question if available.\n",
    "            priority_order = question.get('priority_order')\n",
    "            if priority_order:\n",
    "                # If it's a list, join the elements with a comma.\n",
    "                if isinstance(priority_order, list):\n",
    "                    order_str = \", \".join(priority_order)\n",
    "                else:\n",
    "                    order_str = str(priority_order)\n",
    "                f.write(f\"**Priority Order**: {order_str}\\n\\n\")\n",
    "    \n",
    "    with open(file_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        # Write a header that includes the company name.\n",
    "        f.write(f\"# Fact Sheet -- **{COMPANY_NAME}**\\n\\n\")\n",
    "        \n",
    "        # Iterate through each section\n",
    "        for i, section in enumerate(data.get('sections', []), start=1):\n",
    "            section_title = section.get('section_title', f\"Section {i}\")\n",
    "            section_anchor = f\"section-{i}\"\n",
    "            f.write(f\"## <a id='{section_anchor}'></a> {section_title}\\n\\n\")\n",
    "            \n",
    "            # Each section has a list under \"content\". In the provided JSON these items contain the questions.\n",
    "            for j, content_item in enumerate(section.get('content', []), start=1):\n",
    "                # Optionally write a subtitle if available.\n",
    "                subtitle = content_item.get('title')\n",
    "                if subtitle:\n",
    "                    content_anchor = f\"{section_anchor}-content-{j}\"\n",
    "                    f.write(f\"### <a id='{content_anchor}'></a> {subtitle}\\n\\n\")\n",
    "                \n",
    "                # Write out any questions found in this content block.\n",
    "                if 'questions' in content_item:\n",
    "                    write_questions(f, content_item['questions'])\n",
    "                \n",
    "                # If there are nested subsections, write them as well.\n",
    "                for k, sub_item in enumerate(content_item.get('subsections', []), start=1):\n",
    "                    sub_title = sub_item.get('title', f\"Subsection {k}\")\n",
    "                    sub_anchor = f\"{section_anchor}-subsection-{k}\"\n",
    "                    f.write(f\"#### <a id='{sub_anchor}'></a> {sub_title}\\n\\n\")\n",
    "                    \n",
    "                    if 'questions' in sub_item:\n",
    "                        write_questions(f, sub_item['questions'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d1cd452",
   "metadata": {},
   "outputs": [],
   "source": [
    "COMPANY_NAME = \"Euhearing Therapeutics\"\n",
    "now = datetime.now(pytz.timezone('Asia/Shanghai')).date()\n",
    "#now = '2025-05-15'\n",
    "\n",
    "OUTPUT_PATH_PROMPTS = f\"{COMPANY_NAME}_factsheet_prompting_questions_{now}.md\"   \n",
    "\n",
    "write_prompts(OUTPUT_PATH_PROMPTS, json_questions_string)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcb25bb7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e50d7e71",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To update a section's response\n",
    "company_name = \"PPInnova (Peak Perform Innova)\"\n",
    "\n",
    "# Path to your existing responses\n",
    "now = datetime.now(pytz.timezone('Asia/Shanghai')).date()\n",
    "existing_responses_path = f\"all_sections_responses_gpt41_factsheet_{now}.pkl\"\n",
    "\n",
    "updated_responses = update_any_section_or_subsection(\n",
    "    json_questions_string=json_questions_string,\n",
    "    existing_responses_path=existing_responses_path,\n",
    "    section_title=\"SECTION 1. Overview\",\n",
    "    subsection_title=None,\n",
    "    db_jd=db_jd,\n",
    "    bm25_retriever=bm25_retriever\n",
    ")\n",
    "\n",
    "# Save the updated responses\n",
    "updated_file_path = f\"all_sections_responses_gpt41_factsheet_{now}.pkl\"\n",
    "with open(updated_file_path, \"wb\") as file:\n",
    "    pickle.dump(updated_responses, file)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "896bf01f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To update a section's response\n",
    "company_name = \"Euhearing Therapeutics\"\n",
    "\n",
    "# Path to your existing responses\n",
    "now = datetime.now(pytz.timezone('Asia/Shanghai')).date()\n",
    "existing_responses_path = f\"all_sections_responses_gpt41_factsheet_{now}.pkl\"\n",
    "\n",
    "updated_responses = update_any_section_or_subsection(\n",
    "    json_questions_string=json_questions_string,\n",
    "    existing_responses_path=existing_responses_path,\n",
    "    section_title=\"SECTION 2. Deal dynamics\",\n",
    "    subsection_title=None,\n",
    "    db_jd=db_jd,\n",
    "    bm25_retriever=bm25_retriever\n",
    ")\n",
    "\n",
    "# Save the updated responses\n",
    "updated_file_path = f\"all_sections_responses_gpt41_factsheet_{now}.pkl\"\n",
    "with open(updated_file_path, \"wb\") as file:\n",
    "    pickle.dump(updated_responses, file)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9146b22f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To update a section's response\n",
    "company_name = \"Euhearing Therapeutics\"\n",
    "\n",
    "# Path to your existing responses\n",
    "now = datetime.now(pytz.timezone('Asia/Shanghai')).date()\n",
    "existing_responses_path = f\"all_sections_responses_gpt41_factsheet_{now}.pkl\"\n",
    "\n",
    "updated_responses = update_any_section_or_subsection(\n",
    "    json_questions_string=json_questions_string,\n",
    "    existing_responses_path=existing_responses_path,\n",
    "    section_title=\"SECTION 4. Product summary and pipeline analysis\",\n",
    "    subsection_title=\"Competitive landscape and potential strategic interest\",\n",
    "    db_jd=db_jd,\n",
    "    bm25_retriever=bm25_retriever\n",
    ")\n",
    "\n",
    "# Save the updated responses\n",
    "updated_file_path = f\"all_sections_responses_gpt41_factsheet_{now}.pkl\"\n",
    "with open(updated_file_path, \"wb\") as file:\n",
    "    pickle.dump(updated_responses, file)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5655a164",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To update a section's response\n",
    "company_name = \"Euhearing Therapeutics\"\n",
    "\n",
    "# Path to your existing responses\n",
    "#now = datetime.now(pytz.timezone('Asia/Shanghai')).date()\n",
    "existing_responses_path = f\"all_sections_responses_gpt41_factsheet_2025-08-26.pkl\"\n",
    "\n",
    "updated_responses = update_any_section_or_subsection(\n",
    "    json_questions_string=json_questions_string,\n",
    "    existing_responses_path=existing_responses_path,\n",
    "    section_title=\"Appendix\",\n",
    "    subsection_title=\"Market analysis by AI\",\n",
    "    db_jd=db_jd,\n",
    "    bm25_retriever=bm25_retriever\n",
    ")\n",
    "\n",
    "# Save the updated responses\n",
    "updated_file_path = f\"all_sections_responses_gpt41_factsheet_{now}.pkl\"\n",
    "with open(updated_file_path, \"wb\") as file:\n",
    "    pickle.dump(updated_responses, file)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2db5b387",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To update a section's response\n",
    "company_name = \"Euhearing Therapeutics\"\n",
    "\n",
    "# Path to your existing responses\n",
    "#now = datetime.now(pytz.timezone('Asia/Shanghai')).date()\n",
    "existing_responses_path = f\"all_sections_responses_gpt41_factsheet_2025-08-26.pkl\"\n",
    "\n",
    "updated_responses = update_any_section_or_subsection(\n",
    "    json_questions_string=json_questions_string,\n",
    "    existing_responses_path=existing_responses_path,\n",
    "    section_title=\"SECTION 3. Platform\",\n",
    "    subsection_title=None,\n",
    "    db_jd=db_jd,\n",
    "    bm25_retriever=bm25_retriever\n",
    ")\n",
    "\n",
    "# Save the updated responses\n",
    "updated_file_path = f\"all_sections_responses_gpt41_factsheet_{now}.pkl\"\n",
    "with open(updated_file_path, \"wb\") as file:\n",
    "    pickle.dump(updated_responses, file)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae760b5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "COMPANY_NAME = \"PPInnova\"\n",
    "now = datetime.now(pytz.timezone('Asia/Shanghai')).date()\n",
    "#now = '2025-05-15'\n",
    "\n",
    "OUTPUT_PATH_WITH_PROMPTS_WITH_SOURCE = f\"{COMPANY_NAME}_factsheet_with_prompts_with_source_gpt41_{now}.md\"\n",
    "OUTPUT_PATH_WITH_PROMPTS_WITHOUT_SOURCE = f\"{COMPANY_NAME}_factsheet_with_prompts_without_source_gpt41_{now}.md\"\n",
    "OUTPUT_PATH_WITHOUT_PROMPTS_WITH_SOURCE = f\"{COMPANY_NAME}_factsheet_with_source_gpt41_{now}.md\"\n",
    "OUTPUT_PATH_WITHOUT_PROMPTS_WITHOUT_SOURCE = f\"{COMPANY_NAME}_factsheet_without_source_gpt41_{now}.md\"\n",
    "\n",
    "image_dir = f\"{os.getcwd()}/images_for_factsheet_generation\"    \n",
    "\n",
    "write_file(OUTPUT_PATH_WITH_PROMPTS_WITH_SOURCE, include_sources=True, include_prompts=True, all_sections_responses=updated_responses)\n",
    "write_file(OUTPUT_PATH_WITH_PROMPTS_WITHOUT_SOURCE, include_sources=False, include_prompts=True, all_sections_responses=updated_responses)\n",
    "write_file(OUTPUT_PATH_WITHOUT_PROMPTS_WITH_SOURCE, include_sources=True, include_prompts=False, all_sections_responses=updated_responses)\n",
    "write_file(OUTPUT_PATH_WITHOUT_PROMPTS_WITHOUT_SOURCE, include_sources=False, include_prompts=False, all_sections_responses=updated_responses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16977c6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "COMPANY_NAME = \"Euhearing Therapeutics\"\n",
    "now = datetime.now(pytz.timezone('Asia/Shanghai')).date()\n",
    "#now = '2025-05-15'\n",
    "\n",
    "OUTPUT_PATH_WITH_PROMPTS_WITH_SOURCE = f\"{COMPANY_NAME}_factsheet_with_prompts_with_source_gpt41_{now}_updated.md\"\n",
    "OUTPUT_PATH_WITH_PROMPTS_WITHOUT_SOURCE = f\"{COMPANY_NAME}_factsheet_with_prompts_without_source_gpt41_{now}_updated.md\"\n",
    "OUTPUT_PATH_WITHOUT_PROMPTS_WITH_SOURCE = f\"{COMPANY_NAME}_factsheet_with_source_gpt41_{now}_updated.md\"\n",
    "OUTPUT_PATH_WITHOUT_PROMPTS_WITHOUT_SOURCE = f\"{COMPANY_NAME}_factsheet_without_source_gpt41_{now}_updated.md\"\n",
    "\n",
    "image_dir = f\"{os.getcwd()}/images_for_factsheet_generation\"    \n",
    "\n",
    "write_file(OUTPUT_PATH_WITH_PROMPTS_WITH_SOURCE, include_sources=True, include_prompts=True, all_sections_responses=updated_responses)\n",
    "write_file(OUTPUT_PATH_WITH_PROMPTS_WITHOUT_SOURCE, include_sources=False, include_prompts=True, all_sections_responses=updated_responses)\n",
    "write_file(OUTPUT_PATH_WITHOUT_PROMPTS_WITH_SOURCE, include_sources=True, include_prompts=False, all_sections_responses=updated_responses)\n",
    "write_file(OUTPUT_PATH_WITHOUT_PROMPTS_WITHOUT_SOURCE, include_sources=False, include_prompts=False, all_sections_responses=updated_responses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fe37afa",
   "metadata": {},
   "outputs": [],
   "source": [
    "COMPANY_NAME = \"Euhearing Therapeutics\"\n",
    "now = datetime.now(pytz.timezone('Asia/Shanghai')).date()\n",
    "#now = '2025-05-15'\n",
    "\n",
    "OUTPUT_PATH_PROMPTS = f\"{COMPANY_NAME}_factsheet_prompting_questions_{now}.md\"   \n",
    "\n",
    "write_prompts(OUTPUT_PATH_PROMPTS, json_questions_string)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cc66b0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_only_direct_questions(json_questions_string, existing_responses_path,\n",
    "                                section_title, db_jd, bm25_retriever):\n",
    "    \"\"\"\n",
    "    Update only the direct questions in a mixed section, preserving all subsections.\n",
    "    \"\"\"\n",
    "    # Load existing responses\n",
    "    with open(existing_responses_path, \"rb\") as file:\n",
    "        all_sections_responses = pickle.load(file)\n",
    "    \n",
    "    # Parse questions to find direct questions only\n",
    "    json_questions = json.loads(json_questions_string)\n",
    "    \n",
    "    # Extract ONLY the direct questions from this section\n",
    "    direct_questions_only = []\n",
    "    for section in json_questions[\"sections\"]:\n",
    "        if section[\"section_title\"] == section_title:\n",
    "            for item in section[\"content\"]:\n",
    "                # Only get items with questions but no title (direct questions)\n",
    "                if \"questions\" in item and \"title\" not in item:\n",
    "                    direct_questions_only.append(item)\n",
    "            break\n",
    "    \n",
    "    if not direct_questions_only:\n",
    "        print(\"No direct questions found\")\n",
    "        return all_sections_responses\n",
    "    \n",
    "    # Re-run ONLY the direct questions\n",
    "    new_direct_responses = gather_responses(direct_questions_only, db_jd, bm25_retriever)\n",
    "    \n",
    "    # Find and update the section, preserving subsections\n",
    "    for section in all_sections_responses:\n",
    "        if section[\"section_title\"] == section_title:\n",
    "            # Get the existing content\n",
    "            existing_content = section[\"content\"][0]  # The main content tuple\n",
    "            \n",
    "            # Extract the existing subsections (index 3 in the tuple)\n",
    "            preserved_subsections = existing_content[3]\n",
    "            \n",
    "            # Create updated content with new direct questions but old subsections\n",
    "            updated_content = (\n",
    "                new_direct_responses[0][0],  # title (usually None)\n",
    "                new_direct_responses[0][1],  # new direct questions response with prompt\n",
    "                new_direct_responses[0][2],  # new direct questions response without prompt\n",
    "                preserved_subsections,        # KEEP EXISTING SUBSECTIONS UNCHANGED\n",
    "                new_direct_responses[0][4],  # source counts\n",
    "                new_direct_responses[0][5],  # images\n",
    "                new_direct_responses[0][6],  # include_image flag\n",
    "                new_direct_responses[0][7]   # online search responses\n",
    "            )\n",
    "            \n",
    "            section[\"content\"] = [updated_content]\n",
    "            print(f\"Updated direct questions, preserved {len(preserved_subsections)} subsections\")\n",
    "            break\n",
    "    \n",
    "    return all_sections_responses\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e18c697",
   "metadata": {},
   "outputs": [],
   "source": [
    "existing_responses_path = f\"all_sections_responses_gpt41_factsheet_2025-08-26.pkl\"\n",
    "updated_responses = update_only_direct_questions(\n",
    "    json_questions_string,\n",
    "    existing_responses_path,\n",
    "    \"SECTION 4. Product summary and pipeline analysis\",\n",
    "    db_jd,\n",
    "    bm25_retriever\n",
    ")\n",
    "\n",
    "# Save the updated responses\n",
    "with open(\"updated_responses.pkl\", \"wb\") as f:\n",
    "    pickle.dump(updated_responses, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7c3505f",
   "metadata": {},
   "outputs": [],
   "source": [
    "COMPANY_NAME = \"Euhearing Therapeutics\"\n",
    "now = datetime.now(pytz.timezone('Asia/Shanghai')).date()\n",
    "#now = '2025-05-15'\n",
    "\n",
    "OUTPUT_PATH_WITH_PROMPTS_WITH_SOURCE = f\"{COMPANY_NAME}_factsheet_with_prompts_with_source_gpt41_{now}.md\"\n",
    "OUTPUT_PATH_WITH_PROMPTS_WITHOUT_SOURCE = f\"{COMPANY_NAME}_factsheet_with_prompts_without_source_gpt41_{now}.md\"\n",
    "OUTPUT_PATH_WITHOUT_PROMPTS_WITH_SOURCE = f\"{COMPANY_NAME}_factsheet_with_source_gpt41_{now}.md\"\n",
    "OUTPUT_PATH_WITHOUT_PROMPTS_WITHOUT_SOURCE = f\"{COMPANY_NAME}_factsheet_without_source_gpt41_{now}.md\"\n",
    "\n",
    "image_dir = f\"{os.getcwd()}/images_for_factsheet_generation\"    \n",
    "\n",
    "write_file(OUTPUT_PATH_WITH_PROMPTS_WITH_SOURCE, include_sources=True, include_prompts=True, all_sections_responses=updated_responses)\n",
    "write_file(OUTPUT_PATH_WITH_PROMPTS_WITHOUT_SOURCE, include_sources=False, include_prompts=True, all_sections_responses=updated_responses)\n",
    "write_file(OUTPUT_PATH_WITHOUT_PROMPTS_WITH_SOURCE, include_sources=True, include_prompts=False, all_sections_responses=updated_responses)\n",
    "write_file(OUTPUT_PATH_WITHOUT_PROMPTS_WITHOUT_SOURCE, include_sources=False, include_prompts=False, all_sections_responses=updated_responses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81b9f954",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_p310",
   "language": "python",
   "name": "conda_pytorch_p310"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
