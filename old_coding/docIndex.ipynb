{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a96c08c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#!pip install setuptools==58.0.4\n",
    "!pip install  -r requirements.txt\n",
    "!pip install PyMuPDF==1.26.0\n",
    "!pip install python-docx\n",
    "!pip install Pillow pytesseract\n",
    "!pip install jq\n",
    "!pip install --upgrade --quiet  langchain langchain-community azure-ai-documentintelligence\n",
    "!pip install semchunk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5b2f5d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import langchain\n",
    "from typing import List\n",
    "import boto3\n",
    "import ast\n",
    "\n",
    "#import torch\n",
    "from dotenv import load_dotenv\n",
    "from multiprocessing import Pool\n",
    "\n",
    "import chromadb\n",
    "from chromadb.config import Settings\n",
    "from langchain import FAISS\n",
    "from tqdm import tqdm\n",
    "\n",
    "from docx import Document as DocxDocument\n",
    "from pptx import Presentation as PptxDocument\n",
    "\n",
    "import bs4\n",
    "from langchain import hub\n",
    "from langchain_community.document_loaders import UnstructuredFileLoader,UnstructuredWordDocumentLoader,UnstructuredExcelLoader,JSONLoader\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from langchain.schema import StrOutputParser\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from typing import List, Dict, Any\n",
    "\n",
    "from langchain.document_loaders import (\n",
    "    WebBaseLoader,\n",
    "    CSVLoader,\n",
    "    EverNoteLoader,\n",
    "    PDFMinerLoader,\n",
    "    TextLoader,\n",
    "    UnstructuredEmailLoader,\n",
    "    UnstructuredEPubLoader,\n",
    "    UnstructuredHTMLLoader,\n",
    "    UnstructuredMarkdownLoader,\n",
    "    UnstructuredPowerPointLoader,\n",
    "    UnstructuredWordDocumentLoader,\n",
    "    UnstructuredExcelLoader,\n",
    "    ReadTheDocsLoader,\n",
    "    Docx2txtLoader\n",
    ")\n",
    "\n",
    "from langchain_community.document_loaders import AzureAIDocumentIntelligenceLoader\n",
    "\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.embeddings import HuggingFaceEmbeddings   #, OpenAIEmbeddings\n",
    "from langchain.docstore.document import Document\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('punkt_tab')\n",
    "nltk.download('averaged_perceptron_tagger_eng')\n",
    "\n",
    "import semchunk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb613544",
   "metadata": {},
   "outputs": [],
   "source": [
    "import base64\n",
    "import fitz  \n",
    "import io\n",
    "from PIL import Image\n",
    "from multiprocessing import cpu_count\n",
    "\n",
    "from typing import List\n",
    "from langchain_community.document_loaders.base import BaseLoader\n",
    "from langchain_core.documents import Document\n",
    "\n",
    "from IPython.display import display, Image\n",
    "import subprocess\n",
    "import json\n",
    "import jq\n",
    "from functools import partial\n",
    "import tempfile\n",
    "import time\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "533ce569",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load_dotenv(\"config.env\")\n",
    "\n",
    "# # persist_directory = './db'\n",
    "# embeddings_model_name = \"BAAI/bge-base-en-v1.5\"\n",
    "# chunk_size = 1000\n",
    "# chunk_overlap = 200\n",
    "\n",
    "\n",
    "# CHROMA_SETTINGS = Settings(\n",
    "# #        chroma_db_impl='duckdb+parquet',\n",
    "#         persist_directory=persist_directory,\n",
    "#         anonymized_telemetry=False\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c04616c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c54c26b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_key(secret_name,region_name):\n",
    "\n",
    "    session = boto3.session.Session()\n",
    "    client = session.client(\n",
    "        service_name='secretsmanager',\n",
    "        region_name=region_name\n",
    "    )\n",
    "\n",
    "    try:\n",
    "        get_secret_value_response = client.get_secret_value(\n",
    "            SecretId=secret_name\n",
    "        )\n",
    "    except ClientError as e:\n",
    "        raise e\n",
    "\n",
    "    secret = get_secret_value_response['SecretString']\n",
    "    key = ast.literal_eval(secret)['key']\n",
    "    \n",
    "    return key\n",
    "\n",
    "openai_api_key=get_key(\"openai-api-key\", \"us-west-2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5599c03",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def get_images_from_page(page, scale_factor: float = 1.0, image_encoding: str = \"png\") -> list[str]:\n",
    "    \n",
    "#     mat = fitz.Matrix(scale_factor, scale_factor)\n",
    "#     pix = page.get_pixmap(matrix=mat)\n",
    "#     img_bytes = pix.tobytes(output=image_encoding)\n",
    "    \n",
    "#     return base64.b64encode(img_bytes).decode(\"utf-8\")\n",
    "\n",
    "def process_image_with_ai(image_base64: str, model: str) -> str:\n",
    "      \n",
    "    max_retries = 5\n",
    "    attempt = 0\n",
    "    while attempt < max_retries:\n",
    "        try:\n",
    "            client = OpenAI(api_key = get_key(\"openai-api-key\",\"us-west-2\"))\n",
    "            response = client.chat.completions.create(\n",
    "                model=model,\n",
    "                #temperature=0,\n",
    "                messages=[\n",
    "                    {\n",
    "                        \"role\": \"user\",\n",
    "                        \"content\": [\n",
    "                            {\n",
    "                                \"type\": \"text\",\n",
    "                                \"text\": \"Please extract information from the following image in detail and precisely.\",\n",
    "                            },\n",
    "                            {\n",
    "                                \"type\": \"image_url\",\n",
    "                                \"image_url\": {\n",
    "                                    \"url\": f\"data:image/png;base64,{image_base64}\",\n",
    "                                },\n",
    "                            },\n",
    "                        ],\n",
    "                    }\n",
    "                ],\n",
    "                #max_tokens=1024,\n",
    "            )\n",
    "            return response.choices[0].message.content\n",
    "        \n",
    "        except Exception as e:\n",
    "            attempt += 1\n",
    "            if attempt == max_retries:\n",
    "                return f\"Error: {str(e)}\"\n",
    "            time.sleep(2)  # Wait for 2 seconds before retrying\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bc5954a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_sub_images(document: fitz.Document) -> list[list[dict]]:\n",
    "    images = []\n",
    "    for page in document:\n",
    "        page_images = []\n",
    "        image_list = page.get_images(full=True)\n",
    "        for j, img in enumerate(image_list):\n",
    "            base_image = document.extract_image(img[0])\n",
    "            page_images.append(\n",
    "                {\n",
    "                    \"type\": \"image\",\n",
    "                    \"index\": j,\n",
    "                    \"base64\": base64.b64encode(base_image[\"image\"]).decode(\"utf-8\"),\n",
    "                    \"ext\": base_image[\"ext\"],\n",
    "                }\n",
    "            )\n",
    "        images.append(page_images)\n",
    "    return images\n",
    "\n",
    "\n",
    "# ## extract information from images only when the page has sub images or tables to avoid processing pure text pages thus reduce processing time\n",
    "def extract_information_from_page(page: fitz.Page, page_number: int, model: str, source: str, scale_factor: float = 1.5) -> dict:\n",
    "    text = page.get_text()\n",
    "    combined_text = [f\"Page {page_number + 1} Text:\\n{text}\\n\"]\n",
    "    document = fitz.open(page.parent.name)\n",
    "    sub_images = extract_sub_images(document)\n",
    "    \n",
    "    # Check if page has tables\n",
    "    tables = page.find_tables()\n",
    "    has_tables = len(tables.tables) > 0 if tables else False\n",
    "    \n",
    "    # Check if page has sub-images\n",
    "    has_sub_images = bool(sub_images[page_number])\n",
    "    \n",
    "    # Process as image if page has sub-images OR tables\n",
    "    if has_sub_images or has_tables:\n",
    "        # Increase resolution using a scale factor=1.5 or 2.0\n",
    "        mat = fitz.Matrix(scale_factor, scale_factor)\n",
    "        pix = page.get_pixmap(matrix=mat, alpha=False)\n",
    "        overview_image_base64 = base64.b64encode(pix.tobytes(output=\"png\")).decode(\"utf-8\")\n",
    "        \n",
    "        # Process the page image with AI\n",
    "        processed_text = process_image_with_ai(overview_image_base64, model)\n",
    "        combined_text.append(f\"Page {page_number + 1} Image Information:\\n{processed_text}\\n\")\n",
    "        \n",
    "        result = {\n",
    "            \"text\": \"\\n\".join(combined_text),\n",
    "            \"metadata\": {\n",
    "                \"overview_image\": overview_image_base64,\n",
    "                \"source\": source,\n",
    "                \"seq_num\": page_number + 1\n",
    "            }\n",
    "        }\n",
    "    else:  # Only plain text on the page\n",
    "        result = {\n",
    "            \"text\": \"\\n\".join(combined_text),\n",
    "            \"metadata\": {\n",
    "                \"source\": source,\n",
    "                \"seq_num\": page_number + 1\n",
    "            }\n",
    "        }\n",
    "    \n",
    "    return result\n",
    "\n",
    "\n",
    "# ## extract information from images only when the page has sub images to avoid processing pure text pages thus reduce processing time\n",
    "# def extract_information_from_page(page: fitz.Page, page_number: int, model: str, source: str, scale_factor: float = 1.5) -> dict:\n",
    "#     text = page.get_text()\n",
    "#     combined_text = [f\"Page {page_number + 1} Text:\\n{text}\\n\"]\n",
    "\n",
    "#     document = fitz.open(page.parent.name)\n",
    "#     sub_images = extract_sub_images(document)\n",
    "    \n",
    "#     # Increase resolution using a scale factor=1.5 or 2.0\n",
    "#     mat = fitz.Matrix(scale_factor, scale_factor)  # Higher scale = better resolution\n",
    "#     pix = page.get_pixmap(matrix=mat, alpha=False)  # Ensure alpha=False for consistent encoding\n",
    "\n",
    "#     if sub_images[page_number]:  # Check if there are sub-images on the page\n",
    "#         overview_image_base64 = base64.b64encode(pix.tobytes(output=\"png\")).decode(\"utf-8\")\n",
    "#         processed_text = process_image_with_ai(overview_image_base64, model)\n",
    "#         combined_text.append(f\"Page {page_number + 1} Image Information:\\n{processed_text}\\n\")\n",
    "\n",
    "#         result = {\n",
    "#             \"text\": \"\\n\".join(combined_text),\n",
    "#             \"metadata\": {\n",
    "#                 \"overview_image\": overview_image_base64,\n",
    "#                 \"sub_images\": ', '.join([img[\"base64\"] for img in sub_images[page_number]]),\n",
    "#                 \"source\": source,\n",
    "#                 \"seq_num\": page_number + 1\n",
    "#             }\n",
    "#         }\n",
    "#     else:  # Only text on the page\n",
    "#         result = {\n",
    "#             \"text\": \"\\n\".join(combined_text),\n",
    "#             \"metadata\": {\n",
    "#                 \"source\": source,\n",
    "#                 \"seq_num\": page_number + 1\n",
    "#             }\n",
    "#         }\n",
    "    \n",
    "#     return result\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# ## extract information from images for all pdf page to ensure maxiumn content and slightly different content for same pdf with different filenames\n",
    "# def extract_information_from_page(page: fitz.Page, page_number: int, model: str, source: str) -> dict:\n",
    "#     text = page.get_text()\n",
    "#     combined_text = [f\"Page {page_number + 1} Text:\\n{text}\\n\"]\n",
    "\n",
    "#     document = fitz.open(page.parent.name)\n",
    "#     sub_images = extract_sub_images(document)\n",
    "\n",
    "#     overview_image_base64 = base64.b64encode(page.get_pixmap().tobytes(output=\"png\")).decode(\"utf-8\")\n",
    "#     gpt4_text = process_image_with_gpt4(overview_image_base64, model)\n",
    "#     combined_text.append(f\"Page {page_number + 1} Image Information:\\n{gpt4_text}\\n\")\n",
    "\n",
    "#     result = {\n",
    "#         \"text\": \"\\n\".join(combined_text),\n",
    "#         \"metadata\": {\n",
    "#             \"overview_image\": overview_image_base64,\n",
    "#             \"sub_images\": ', '.join([img[\"base64\"] for img in sub_images[page_number]]),\n",
    "#             \"source\": source,\n",
    "#             \"seq_num\": page_number + 1\n",
    "#         }\n",
    "#     }\n",
    "    \n",
    "#     return result\n",
    "\n",
    "\n",
    "\n",
    "### multi-processing pages in a pdf\n",
    "def helper(page_number, pdf_path, model):\n",
    "    doc = fitz.open(pdf_path)\n",
    "    page = doc.load_page(page_number)\n",
    "    return extract_information_from_page(page, page_number, model, pdf_path)\n",
    "\n",
    "def extract_information_from_pdf(pdf_path: str, model: str = \"gpt-4o\") -> str:\n",
    "    doc = fitz.open(pdf_path)\n",
    "    num_pages = len(doc)\n",
    "    \n",
    "    args = [(i, pdf_path, model) for i in range(num_pages)]\n",
    "    \n",
    "    try:\n",
    "        with Pool(processes=cpu_count()) as pool:\n",
    "            results = pool.starmap(helper, args)\n",
    "    \n",
    "        extracted_data = [{\n",
    "            \"page_content\": result[\"text\"],\n",
    "            \"metadata\": result[\"metadata\"]\n",
    "        } for result in results]\n",
    "    \n",
    "        return json.dumps(extracted_data, ensure_ascii=False, indent=4)\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred during multiprocessing: {e}\")\n",
    "        results = []\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f19deec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import fitz\n",
    "# # Open and save the PDF to potentially fix issues\n",
    "# pdf_path=\"datapack-Euhearing_20250817091531.pdf\"\n",
    "# doc = fitz.open(pdf_path)\n",
    "# doc.save(\"repaired_datapack-Euhearing_20250817091531.pdf\", garbage=4, deflate=True, clean=True)\n",
    "# doc.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40eecbf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pdf_path=\"repaired_datapack-Euhearing_20250817091531.pdf\"\n",
    "# doc = fitz.open(pdf_path)\n",
    "# text = doc[1].get_text()\n",
    "# text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4070ac5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# results=extract_information_from_pdf(\"datapack-Euhearing_20250817091531.pdf\")\n",
    "\n",
    "# data = json.loads(results)[16:17]\n",
    "\n",
    "# for page_info in data:\n",
    "#     # Display the text content\n",
    "#     print(page_info[\"page_content\"])\n",
    "    \n",
    "#     # Display the overview image\n",
    "#     if 'overview_image' in page_info[\"metadata\"]:\n",
    "#         print(\"overview_image:\")\n",
    "#         display(Image(data=base64.b64decode(page_info[\"metadata\"][\"overview_image\"])))\n",
    "    \n",
    "#     # Display sub-images if available\n",
    "#     if 'sub_images' in page_info[\"metadata\"]:\n",
    "#         print(\"sub_images:\")\n",
    "#         sub_image=page_info[\"metadata\"][\"sub_images\"].split(', ')\n",
    "#         for img_base64 in sub_image:\n",
    "#             display(Image(data=base64.b64decode(img_base64)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7eb8401c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_json_data(json_path: str):\n",
    "    with open(json_path, 'r', encoding='utf-8') as f:\n",
    "        data = json.load(f)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef364a52",
   "metadata": {},
   "outputs": [],
   "source": [
    "##load source files from S3 and save the processed file to S3\n",
    "def ensure_s3_directory_exists(s3_client, s3_bucket: str, s3_output_dir: str):\n",
    "    if not s3_output_dir.endswith('/'):\n",
    "        s3_output_dir += '/'\n",
    "    try:\n",
    "        s3_client.put_object(Bucket=s3_bucket, Key=s3_output_dir)\n",
    "        print(f\"Ensured S3 directory s3://{s3_bucket}/{s3_output_dir} exists.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error ensuring S3 directory exists: {e}\")\n",
    "\n",
    "        \n",
    "def process_pdfs_in_s3_folder(s3_source_bucket: str, s3_source_folder: str, s3_output_bucket: str, s3_output_folder: str, model: str = \"gpt-4o\"):\n",
    "    s3_client = boto3.client('s3')\n",
    "    \n",
    "    # Ensure the S3 output directory exists\n",
    "    ensure_s3_directory_exists(s3_client, s3_output_bucket, s3_output_folder)\n",
    "    \n",
    "    response = s3_client.list_objects_v2(Bucket=s3_source_bucket, Prefix=s3_source_folder)\n",
    "    pdf_files = [obj['Key'] for obj in response.get('Contents', []) if obj['Key'].lower().endswith('.pdf')]\n",
    "\n",
    "    for pdf_key in pdf_files:\n",
    "        print(f\"Processing s3://{s3_source_bucket}/{pdf_key}\")\n",
    "        output_file_base = os.path.splitext(os.path.basename(pdf_key))[0]\n",
    "        # Download the PDF file to a temporary file\n",
    "        temp_pdf_path = f\"/tmp/{os.path.basename(pdf_key)}\"\n",
    "        s3_client.download_file(s3_source_bucket, pdf_key, temp_pdf_path)\n",
    "        \n",
    "        # Process the PDF\n",
    "        extracted_info = extract_information_from_pdf(temp_pdf_path, model)\n",
    "        \n",
    "        # Check and print the extracted info for debugging\n",
    "        if extracted_info is None:\n",
    "            print(f\"No information extracted from {pdf_key}.\")\n",
    "            continue\n",
    "        \n",
    "        # Save the extracted information to a temporary file\n",
    "        output_file_name = output_file_base + '.json'\n",
    "        temp_json_path = f\"/tmp/{output_file_name}\"\n",
    "        with open(temp_json_path, 'w', encoding='utf-8') as f:\n",
    "            f.write(extracted_info)\n",
    "        \n",
    "        # Upload the temporary JSON file to S3\n",
    "        s3_key = os.path.join(s3_output_folder, output_file_name)\n",
    "        s3_client.upload_file(temp_json_path, s3_output_bucket, s3_key)\n",
    "        \n",
    "        # Remove the temporary files\n",
    "        os.remove(temp_pdf_path)\n",
    "        os.remove(temp_json_path)\n",
    "        \n",
    "        print(f\"Saved extracted information to s3://{s3_output_bucket}/{s3_key}\")\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5b394aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_unprocessed_pdfs_in_s3_folder(s3_source_bucket: str, s3_source_folder: str, s3_output_bucket: str, s3_output_folder: str, model: str = \"gpt-4o\"):\n",
    "    s3_client = boto3.client('s3')\n",
    "    \n",
    "    # Ensure the S3 output directory exists\n",
    "    ensure_s3_directory_exists(s3_client, s3_output_bucket, s3_output_folder)\n",
    "    \n",
    "    # List all PDF files in the source S3 folder\n",
    "    response = s3_client.list_objects_v2(Bucket=s3_source_bucket, Prefix=s3_source_folder)\n",
    "    pdf_files = [obj['Key'] for obj in response.get('Contents', []) if obj['Key'].lower().endswith('.pdf')]\n",
    "    \n",
    "    # List all processed JSON files in the output S3 folder\n",
    "    response = s3_client.list_objects_v2(Bucket=s3_output_bucket, Prefix=s3_output_folder)\n",
    "    processed_files = {os.path.splitext(os.path.basename(obj['Key']))[0] for obj in response.get('Contents', []) if obj['Key'].endswith('.json')}\n",
    "    \n",
    "    for pdf_key in pdf_files:\n",
    "        output_file_base = os.path.splitext(os.path.basename(pdf_key))[0]\n",
    "        if output_file_base in processed_files:\n",
    "            print(f\"Skipping already processed file: s3://{s3_source_bucket}/{pdf_key}\")\n",
    "            continue\n",
    "        \n",
    "        print(f\"Processing s3://{s3_source_bucket}/{pdf_key}\")\n",
    "        \n",
    "        # Download the PDF file to a temporary file\n",
    "        temp_pdf_path = f\"/tmp/{os.path.basename(pdf_key)}\"\n",
    "        s3_client.download_file(s3_source_bucket, pdf_key, temp_pdf_path)\n",
    "        \n",
    "        # Process the PDF\n",
    "        extracted_info = extract_information_from_pdf(temp_pdf_path, model)\n",
    "        \n",
    "        # Check and print the extracted info for debugging\n",
    "        if extracted_info is None:\n",
    "            print(f\"No information extracted from {pdf_key}.\")\n",
    "            continue\n",
    "        \n",
    "        # Save the extracted information to a temporary file\n",
    "        output_file_name = output_file_base + '.json'\n",
    "        temp_json_path = f\"/tmp/{output_file_name}\"\n",
    "        with open(temp_json_path, 'w', encoding='utf-8') as f:\n",
    "            f.write(extracted_info)\n",
    "        \n",
    "        # Upload the temporary JSON file to S3\n",
    "        s3_key = os.path.join(s3_output_folder, output_file_name)\n",
    "        s3_client.upload_file(temp_json_path, s3_output_bucket, s3_key)\n",
    "        \n",
    "        # Remove the temporary files\n",
    "        os.remove(temp_pdf_path)\n",
    "        os.remove(temp_json_path)\n",
    "        \n",
    "        print(f\"Saved extracted information to s3://{s3_output_bucket}/{s3_key}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c39a973",
   "metadata": {},
   "outputs": [],
   "source": [
    "def copy_files_from_s3_to_s3(source_bucket: str, source_prefix: str, destination_bucket: str, destination_prefix: str):\n",
    "    \"\"\"\n",
    "    Copies all files from source_prefix in source_bucket to destination_prefix in destination_bucket.\n",
    "    \"\"\"\n",
    "    s3_client = boto3.client('s3')\n",
    "\n",
    "    # List all objects in the source directory\n",
    "    response = s3_client.list_objects_v2(Bucket=source_bucket, Prefix=source_prefix)\n",
    "    if 'Contents' not in response:\n",
    "        print(f\"No files found in {source_bucket}/{source_prefix}\")\n",
    "        return\n",
    "\n",
    "    for obj in response['Contents']:\n",
    "        source_key = obj['Key']\n",
    "        destination_key = source_key.replace(source_prefix, destination_prefix, 1)\n",
    "        \n",
    "        copy_source = {\n",
    "            'Bucket': source_bucket,\n",
    "            'Key': source_key\n",
    "        }\n",
    "        \n",
    "        print(f\"Copying {source_bucket}/{source_key} to {destination_bucket}/{destination_key}\")\n",
    "        s3_client.copy(copy_source, destination_bucket, destination_key)\n",
    "\n",
    "    print(f\"Copied files from s3://{source_bucket}/{source_prefix} to s3://{destination_bucket}/{destination_prefix}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59541e76",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Map file extensions to document loaders and their arguments\n",
    "LOADER_MAPPING = {\n",
    "    \".csv\": (CSVLoader, {}),\n",
    "    '.eml':(UnstructuredFileLoader,{}),\n",
    "    \".html\": (UnstructuredHTMLLoader, {}),\n",
    "    \".md\": (UnstructuredMarkdownLoader, {}),\n",
    "    #\".pdf\": (PDFMinerLoader, {}),   #UnstructuredFileLoader\n",
    "    #\".pdf\": (CustomPDFLoader, {\"model\": \"gpt-4o\"}),\n",
    "    \".txt\": (UnstructuredFileLoader, {\"encoding\": \"utf8\"}), #TextLoader\n",
    "    '.xlsx':(UnstructuredExcelLoader,{}),      #UnstructuredFileLoader  #\"mode\": \"elements\"\n",
    "    #'.XLSX':(UnstructuredExcelLoader,{}), \n",
    "    '.docx':(Docx2txtLoader,{}),\n",
    "    #'.DOCX':(Docx2txtLoader,{}),\n",
    "    #'.pptx':(UnstructuredPowerPointLoader,{}),      \n",
    "    #'.json':(JSONLoader,{\"jq_schema\": '.[] | {text: .text, images: .images}', \"text_content\": False})\n",
    "    # Add more mappings for other file extensions and loaders as needed\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59edc26f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the S3 client\n",
    "s3_client = boto3.client('s3')\n",
    "\n",
    "    \n",
    "# def write_to_temp_file(content: bytes, s3_key: str) -> str:\n",
    "#     # Extract the filename from the S3 key\n",
    "#     filename = os.path.basename(s3_key)\n",
    "#     # Create a temporary file in the current working directory with the extracted filename\n",
    "#     temp_file_path = os.path.join(os.getcwd(), filename)\n",
    "    \n",
    "#     # Write the content to the temporary file\n",
    "#     with open(temp_file_path, 'wb') as temp_file:\n",
    "#         temp_file.write(content)\n",
    "    \n",
    "#     return temp_file_path\n",
    "\n",
    "def write_to_temp_file(content: bytes, s3_key: str) -> str:\n",
    "    # Extract the filename from the S3 key\n",
    "    filename = os.path.basename(s3_key)\n",
    "    \n",
    "    # Define the path to the tmp directory within the current working directory\n",
    "    temp_dir = os.path.join(os.getcwd(), 'tmp')\n",
    "    \n",
    "    # Ensure the tmp directory exists\n",
    "    os.makedirs(temp_dir, exist_ok=True)\n",
    "    \n",
    "    # Create the temporary file path in the tmp directory\n",
    "    temp_file_path = os.path.join(temp_dir, filename)\n",
    "    \n",
    "    # Write the content to the temporary file\n",
    "    with open(temp_file_path, 'wb') as temp_file:\n",
    "        temp_file.write(content)\n",
    "    \n",
    "    return temp_file_path\n",
    "\n",
    "\n",
    "\n",
    "def load_single_document_from_s3(s3_bucket: str, s3_key: str) -> List[Document]:\n",
    "    ext = \".\" + s3_key.rsplit(\".\", 1)[-1]\n",
    "    filename = os.path.basename(s3_key)\n",
    "    \n",
    "    retries = 5  # retry loading if loading error happened, after 5 attempts, skip the file\n",
    "    for attempt in range(retries):\n",
    "        if ext.lower() in LOADER_MAPPING:\n",
    "            try:\n",
    "                loader_class, loader_args = LOADER_MAPPING[ext.lower()]\n",
    "                obj = s3_client.get_object(Bucket=s3_bucket, Key=s3_key)\n",
    "                file_content = obj['Body'].read()\n",
    "                \n",
    "                # Write to a temporary file with the same name as in S3\n",
    "                temp_file_path = write_to_temp_file(file_content, s3_key)\n",
    "                loader = loader_class(temp_file_path, **loader_args)\n",
    "                \n",
    "                # Load the document\n",
    "                try:\n",
    "                    documents = loader.load()\n",
    "                    # Update document metadata to include only the filename\n",
    "                    for document in documents:\n",
    "                        document.metadata['source'] = filename\n",
    "                    return documents\n",
    "                except Exception as e:\n",
    "                    print(f\"Error loading content from {temp_file_path} using loader: {e}\")\n",
    "                    documents = []\n",
    "                finally:\n",
    "                    # Clean up the temporary file\n",
    "                    os.remove(temp_file_path)\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"Error loading {s3_key} from bucket {s3_bucket}: {e}\")\n",
    "                if attempt < retries - 1:\n",
    "                    print(f\"Retrying... ({attempt + 1}/{retries})\")\n",
    "                    continue\n",
    "                else:\n",
    "                    print(f\"Failed to load {s3_key} after {retries} attempts. Skipping.\")\n",
    "                    return []\n",
    "        \n",
    "        elif ext.lower() == '.json':\n",
    "            try:\n",
    "                obj = s3_client.get_object(Bucket=s3_bucket, Key=s3_key)\n",
    "                data = json.loads(obj['Body'].read().decode('utf-8'))\n",
    "                documents = [Document(page_content=item[\"page_content\"], metadata=item[\"metadata\"]) for item in data]                \n",
    "#                 documents: List[Document] = []\n",
    "#                 for item in data:\n",
    "#                     # start from whatever metadata was in the JSON, then override source\n",
    "#                     meta = item.get(\"metadata\", {}).copy()\n",
    "#                     meta[\"source\"] = filename\n",
    "#                     documents.append(Document(page_content=item[\"page_content\"], metadata=meta))                \n",
    "                return documents\n",
    "            except Exception as e:\n",
    "                print(f\"Error loading JSON {s3_key} from bucket {s3_bucket}: {e}\")\n",
    "                if attempt < retries - 1:\n",
    "                    print(f\"Retrying... ({attempt + 1}/{retries})\")\n",
    "                    continue\n",
    "                else:\n",
    "                    print(f\"Failed to load {s3_key} after {retries} attempts. Skipping.\")\n",
    "                    return []\n",
    "        \n",
    "        elif ext.lower() == '.pptx':\n",
    "            try:\n",
    "                obj = s3_client.get_object(Bucket=s3_bucket, Key=s3_key)\n",
    "                file_content = obj['Body'].read()\n",
    "                \n",
    "                # Write to a temporary file with the same name as in S3\n",
    "                temp_file_path = write_to_temp_file(file_content, s3_key)\n",
    "                \n",
    "                # Create the Azure AI Document Intelligence loader\n",
    "                azure_endpoint = \"https://doc-extraction-test.cognitiveservices.azure.com/\"\n",
    "                azure_api_key = get_key(\"azure-doc-extraction-test-key\", \"us-west-2\")\n",
    "                \n",
    "                loader = AzureAIDocumentIntelligenceLoader(\n",
    "                    api_endpoint=azure_endpoint,\n",
    "                    api_key=azure_api_key,\n",
    "                    file_path=temp_file_path,\n",
    "                    #api_version=azure_api_version,\n",
    "                    api_model=\"prebuilt-layout\",\n",
    "                    mode=\"markdown\")\n",
    "                        \n",
    "                try:\n",
    "                    documents = loader.load()\n",
    "                    # Create new documents with only the source metadata and page content\n",
    "                    cleaned_documents = []\n",
    "                    for document in documents:\n",
    "                        cleaned_document = Document(page_content=document.page_content,\n",
    "                                                    metadata={\"source\": filename})\n",
    "                        cleaned_documents.append(cleaned_document)\n",
    "                    return cleaned_documents\n",
    "                except Exception as e:\n",
    "                    print(f\"Error loading content from {temp_file_path} using Azure AI: {e}\")\n",
    "                    documents = []\n",
    "                finally:\n",
    "                    os.remove(temp_file_path) \n",
    "                    \n",
    "            except Exception as e:\n",
    "                print(f\"Error loading PPTX {s3_key} from bucket {s3_bucket}: {e}\")\n",
    "                if attempt < retries - 1:\n",
    "                    print(f\"Retrying... ({attempt + 1}/{retries})\")\n",
    "                    continue\n",
    "                else:\n",
    "                    print(f\"Failed to load {s3_key} after {retries} attempts. Skipping.\")\n",
    "                    return []\n",
    "\n",
    "        raise ValueError(f\"Unsupported file extension '{ext}'\")\n",
    "\n",
    "\n",
    "    \n",
    "    \n",
    "def load_document_key(args) -> List[Document]:\n",
    "    s3_bucket, s3_key = args\n",
    "    return load_single_document_from_s3(s3_bucket, s3_key)\n",
    "\n",
    "\n",
    "# def load_documents_from_s3(s3_bucket: str, s3_folder: str, ignored_files: List[str] = []) -> List[Document]:\n",
    "#     \"\"\"\n",
    "#     Loads all documents from the source S3 bucket and folder, ignoring specified files.\n",
    "#     Handles pagination if there are more than 1,000 files.\n",
    "#     \"\"\"\n",
    "#     s3_keys = []\n",
    "#     to_load = list(LOADER_MAPPING.keys()) + ['.json','.pptx']\n",
    "    \n",
    "#     # Pagination variables\n",
    "#     continuation_token = None\n",
    "#     while True:\n",
    "#         if continuation_token:\n",
    "#             response = s3_client.list_objects_v2(Bucket=s3_bucket, Prefix=s3_folder, ContinuationToken=continuation_token)\n",
    "#         else:\n",
    "#             response = s3_client.list_objects_v2(Bucket=s3_bucket, Prefix=s3_folder)\n",
    "\n",
    "#         if 'Contents' in response:\n",
    "#             for obj in response['Contents']:\n",
    "#                 if any(obj['Key'].lower().endswith(ext) for ext in to_load) and obj['Key'] not in ignored_files:\n",
    "#                     s3_keys.append(obj['Key'])\n",
    "\n",
    "#         # If there's a continuation token, it means there are more files to retrieve\n",
    "#         continuation_token = response.get('NextContinuationToken')\n",
    "#         if not continuation_token:\n",
    "#             break\n",
    "    \n",
    "#     # Process the files using multiprocessing\n",
    "#     with Pool(processes=os.cpu_count()) as pool:\n",
    "#         results = []\n",
    "#         with tqdm(total=len(s3_keys), desc='Loading new documents', ncols=80) as pbar:\n",
    "#             for i, docs in enumerate(pool.imap_unordered(load_document_key, [(s3_bucket, key) for key in s3_keys])):\n",
    "#                 results.extend(docs)\n",
    "#                 pbar.update()\n",
    "    \n",
    "#     return results\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def load_documents_from_s3(s3_bucket: str, s3_folder: str, ignored_files: List[str] = []) -> List[Document]:\n",
    "    \"\"\"\n",
    "    Loads all documents from the source S3 bucket and folder, ignoring specified files.\n",
    "    Uses a paginator to handle buckets with more than 1,000 objects.\n",
    "    \"\"\"\n",
    "    # extensions we care about\n",
    "    to_load = [ext.lower() for ext in LOADER_MAPPING.keys()] + ['.json', '.pptx']\n",
    "\n",
    "    # collect all matching keys\n",
    "    s3_keys: List[str] = []\n",
    "    paginator = s3_client.get_paginator('list_objects_v2')\n",
    "    for page in paginator.paginate(Bucket=s3_bucket, Prefix=s3_folder):\n",
    "        for obj in page.get('Contents', []):\n",
    "            key = obj['Key']\n",
    "            key_lower = key.lower()\n",
    "            if any(key_lower.endswith(ext) for ext in to_load) and key not in ignored_files:\n",
    "                s3_keys.append(key)\n",
    "\n",
    "    print(f\"Found {len(s3_keys)} documents to load under s3://{s3_bucket}/{s3_folder}\")\n",
    "\n",
    "    # load them in parallel\n",
    "    documents: List[Document] = []\n",
    "    with Pool(processes=os.cpu_count()) as pool:\n",
    "        with tqdm(total=len(s3_keys), desc=\"Loading documents\", ncols=80) as pbar:\n",
    "            for docs in pool.imap_unordered(load_document_key, [(s3_bucket, key) for key in s3_keys]):\n",
    "                documents.extend(docs)\n",
    "                pbar.update()\n",
    "\n",
    "    return documents\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# def process_documents_from_s3(s3_bucket: str, s3_folder: str, ignored_files: List[str] = []) -> List[Document]:\n",
    "#     \"\"\"\n",
    "#     Load documents from S3 and split in chunks\n",
    "#     \"\"\"\n",
    "#     print(f\"Loading documents from s3://{s3_bucket}/{s3_folder}\")\n",
    "#     documents = load_documents_from_s3(s3_bucket, s3_folder, ignored_files)\n",
    "#     if not documents:\n",
    "#         print(\"No new documents to load\")\n",
    "#         exit(0)\n",
    "#     print(f\"Loaded {len(documents)} new documents from s3://{s3_bucket}/{s3_folder}\")\n",
    "#     text_splitter = RecursiveCharacterTextSplitter(chunk_size=chunk_size, chunk_overlap=chunk_overlap)\n",
    "#     texts = text_splitter.split_documents(documents)\n",
    "#     print(f\"Split into {len(texts)} chunks of text (max. {chunk_size} tokens each)\")\n",
    "#     return texts\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b29b669f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b863576",
   "metadata": {},
   "outputs": [],
   "source": [
    "# s3_bucket = 'plfs-han-llm-experiment'\n",
    "# s3_folder = 'ic-memo-generation/Zenas/docs_ic_memo/'\n",
    "\n",
    "# load_single_document_from_s3(s3_bucket,f\"{s3_folder}Key_LLM_Documents_Series C Cap Table $200M Raise v8 - FINAL.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33df87c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "##the original Literature_InsightsBiopharma_20240223_4211133.xlsx, Literature_SLE ph3 trials globaldata.xlsx, Literature_Barbara_Copy of ms_drugs v2[61].xls  had some filters in excel, can't be loaded, \n",
    "##removed the filter and it can be process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6a5ce59",
   "metadata": {},
   "outputs": [],
   "source": [
    "# s3_bucket = 'plfs-han-llm-experiment'\n",
    "# s3_folder = 'ic-memo-generation/Zenas/docs_ic_memo/'\n",
    "\n",
    "# load_single_document_from_s3(s3_bucket,f\"{s3_folder}Literature_InsightsBiopharma_20240223_4211133.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41265afd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# s3_bucket = 'plfs-han-llm-experiment'\n",
    "# s3_folder = 'ic-memo-generation/Obsidian/docs_ic_memo/'\n",
    "\n",
    "# load_single_document_from_s3(s3_bucket,f\"{s3_folder}KOLs_Dr Amir Jazaeri call notes 07.23.2021.docx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b335aa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# s3_bucket = 'plfs-han-llm-experiment'\n",
    "# s3_folder = 'ic-memo-generation/Obsidian/docs_ic_memo/'\n",
    "# documents = load_single_document_from_s3(s3_bucket,f\"{s3_folder}KOLs_Dr Amir Jazaeri call notes 07.23.2021.docx\")\n",
    "\n",
    "# text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "# texts = text_splitter.split_documents(documents)\n",
    "# texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24fb06ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# s3_bucket = 'plfs-han-llm-experiment'\n",
    "# s3_folder = 'ic-memo-generation/Obsidian/docs_ic_memo/'\n",
    "# documents = load_single_document_from_s3(s3_bucket,f\"{s3_folder}KOLs_Dr Amir Jazaeri call notes 07.23.2021.docx\")\n",
    "\n",
    "# chunker = semchunk.chunkerify('gpt-4', chunk_size=500)\n",
    "\n",
    "# texts = []\n",
    "# for doc in documents:\n",
    "#     chunks = chunker(doc.page_content, overlap=100)\n",
    "#     texts.extend([Document(page_content=chunk, metadata=doc.metadata) for chunk in chunks])\n",
    "#     # Add filename to each chunk after splitting\n",
    "\n",
    "# for chunk in texts:\n",
    "#     filename = chunk.metadata.get('source', 'unknown_file')\n",
    "#     chunk.page_content = f\"From file: {filename}\\n\\n{chunk.page_content}\"\n",
    "# texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "592addd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# s3_bucket = 'plfs-han-llm-experiment'\n",
    "# s3_folder = 'test_bank/xlsx_loader_test/'\n",
    "# documents = load_single_document_from_s3(s3_bucket,f\"{s3_folder}Mosanna_Internal investment process docs_Mosanna DCF v7.xlsx\")\n",
    "\n",
    "# text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "# texts = text_splitter.split_documents(documents)\n",
    "# texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "639a63a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# s3_bucket = 'plfs-han-llm-experiment'\n",
    "# s3_folder = 'test_bank/xlsx_loader_test/'\n",
    "# documents = load_single_document_from_s3(s3_bucket,f\"{s3_folder}Mosanna_Internal investment process docs_Mosanna DCF v7.xlsx\")\n",
    "\n",
    "# text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "# texts = text_splitter.split_documents(documents)\n",
    "# texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f90a38e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# s3_bucket = 'plfs-han-llm-experiment'\n",
    "# s3_folder = 'test_bank/xlsx_loader_test/'\n",
    "# documents = load_single_document_from_s3(s3_bucket,f\"{s3_folder}Mosanna_Internal investment process docs_Mosanna DCF v7.xlsx\")\n",
    "\n",
    "# chunker = semchunk.chunkerify('gpt-4', chunk_size=500)\n",
    "\n",
    "# texts = []\n",
    "# for doc in documents:\n",
    "#     chunks = chunker(doc.page_content, overlap=100)\n",
    "#     texts.extend([Document(page_content=chunk, metadata=doc.metadata) for chunk in chunks])\n",
    "#     # Add filename to each chunk after splitting\n",
    "\n",
    "# for chunk in texts:\n",
    "#     filename = chunk.metadata.get('source', 'unknown_file')\n",
    "#     chunk.page_content = f\"From file: {filename}\\n\\n{chunk.page_content}\"\n",
    "# texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63ff06ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# s3_bucket = 'plfs-han-llm-experiment'\n",
    "# s3_folder = 'test_bank/xlsx_loader_test/'\n",
    "# documents = load_single_document_from_s3(s3_bucket,f\"{s3_folder}Mosanna_Internal investment process docs_Mosanna DCF v7.md\")\n",
    "\n",
    "# chunker = semchunk.chunkerify('gpt-4', chunk_size=500)\n",
    "\n",
    "# texts = []\n",
    "# for doc in documents:\n",
    "#     chunks = chunker(doc.page_content, overlap=100)\n",
    "#     texts.extend([Document(page_content=chunk, metadata=doc.metadata) for chunk in chunks])\n",
    "#     # Add filename to each chunk after splitting\n",
    "\n",
    "# for chunk in texts:\n",
    "#     filename = chunk.metadata.get('source', 'unknown_file')\n",
    "#     chunk.page_content = f\"From file: {filename}\\n\\n{chunk.page_content}\"\n",
    "# texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b368ce44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# s3_bucket = 'plfs-han-llm-experiment'\n",
    "# s3_folder = 'ic-memo-generation/Obsidian/docs_ic_memo/'\n",
    "\n",
    "# documents = load_single_document_from_s3(s3_bucket,f\"{s3_folder}KOLs_Dr Amir Jazaeri call notes 07.23.2021.docx\")\n",
    "\n",
    "# text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=50)\n",
    "# texts = text_splitter.split_documents(documents)\n",
    "\n",
    "# # Add filename to each chunk after splitting\n",
    "# for chunk in texts:\n",
    "#     filename = chunk.metadata.get('source', 'unknown_file')\n",
    "#     chunk.page_content = f\"From file: {filename}\\n\\n{chunk.page_content}\"\n",
    "\n",
    "# texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd9cc4e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# s3_bucket = 'plfs-han-llm-experiment'\n",
    "# s3_folder = 'mini-factsheet-generation/Mosanna'\n",
    "\n",
    "# load_single_document_from_s3(s3_bucket,f\"{s3_folder}/pdf_extraction_with_images_update_metadata/3.1.4.1 AVE0118 Investigator's Brochure 3rd Edition.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3acc312",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load_single_document('./pdf_extraction_with_images/image_extraction_testoutput.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06f5a478",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# documents = load_single_document('./pdf_extraction_with_images/image_extraction_testoutput.json')\n",
    "# text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=50)\n",
    "# texts = text_splitter.split_documents(documents)\n",
    "# texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be99c03d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PDFMinerLoader('./docs_scirhom/2023 Sep_SciRhom_Non-Confidential Deck.pdf').load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffaea121",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load_single_document('./docs_scirhom/2023 Sep_SciRhom_Non-Confidential Deck.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7a493c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# s3_bucket = 'plfs-han-llm-experiment'\n",
    "# s3_key = \"document_search/all_files/Engrail--Board Materials--misc board--Gene TherapyCo BP_draft1.25.pptx\"\n",
    "\n",
    "# load_single_document_from_s3(s3_bucket, s3_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17156b1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# s3_bucket = 'plfs-han-llm-experiment'\n",
    "# s3_key = \"document_search/Mosanna/Mosanna_Internal investment process docs_Mosanna diligence deck.pptx\"\n",
    "\n",
    "# load_single_document_from_s3(s3_bucket, s3_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fd015fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# s3_bucket = 'plfs-han-llm-experiment'\n",
    "# s3_key = \"document_search/all_files/Engrail--Board Materials--misc board--Gene TherapyCo BP_draft1.25.pptx\"\n",
    "\n",
    "# s3_client = boto3.client('s3')\n",
    "# obj = s3_client.get_object(Bucket=s3_bucket, Key=s3_key)\n",
    "# file_content = obj['Body'].read()\n",
    "\n",
    "# # Write to a temporary file with the same name as in S3\n",
    "# temp_file_path = write_to_temp_file(file_content, s3_key)\n",
    "\n",
    "# endpoint = \"https://doc-extraction-test.cognitiveservices.azure.com/\"\n",
    "# key = get_key(\"azure-doc-extraction-test-key\", \"us-west-2\")\n",
    "# loader = AzureAIDocumentIntelligenceLoader(\n",
    "#     api_endpoint=endpoint, api_key=key, file_path=temp_file_path, api_model=\"prebuilt-layout\"\n",
    "# )\n",
    "\n",
    "# documents = loader.load()\n",
    "# documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a217f8df",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_p310",
   "language": "python",
   "name": "conda_pytorch_p310"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
